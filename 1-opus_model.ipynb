{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "import seaborn\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "\n",
    "from gensim_w2v import tokenizers\n",
    "from text_processing_utils import vectorize, build_vocab, get_embeddings, read_fasttext\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains and describes the procedures necessary for pre-training a deep NN on OPUS data and fine-tuning it on the competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-training we will be using the OPUS dataset, freely available for download at http://opus.nlpl.eu/OpenSubtitles2018.php. \n",
    "\n",
    "The repository contains a small sample of the data, consider downloading the whole dataset to get the real results. Russian dataset can be downloaded at http://opus.nlpl.eu/download.php?f=OpenSubtitles2016/en-ru.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read OPUS data\n",
    "# this might as well be english or spanish subtitle data\n",
    "\n",
    "tokens = []\n",
    "c = 0\n",
    "with open(\"./assets/cleaned_subs.txt\", \"r\") as fi:\n",
    "    for l in fi:\n",
    "        tokens.append(l.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough we will use a pre-trained russian fasttext model (not included).\n",
    "\n",
    "You can get one at https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.ru.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read russian fasttext model\n",
    "w2v = read_fasttext(\"./assets/cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n"
     ]
    }
   ],
   "source": [
    "# prepare word-to-id and id-to-word mappings\n",
    "voc, rvoc = build_vocab(tokens, 250000, emb_model=w2v)\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "myembs = get_embeddings(w2v, rvoc)\n",
    "myembs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9016819/9016819 [00:36<00:00, 245561.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# represent data as a matrix of indices \n",
    "VT = vectorize(tokens, voc, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch generator\n",
    "\n",
    "def generate_batch(dmatrix, batch_size=256):\n",
    "\n",
    "    indices = np.arange(0, len(dmatrix)-4)\n",
    "    \n",
    "    def generate_sample():\n",
    "        sid = random.choice(indices)\n",
    "        l = random.choice([0,1])\n",
    "        if l:\n",
    "            # we either use 4 consecutive utterances (positive sample)\n",
    "            sample = dmatrix[sid], dmatrix[sid+1], dmatrix[sid+2], dmatrix[sid+3]\n",
    "        else:\n",
    "            # or 3 consecutive and one random utterance (negative sample)\n",
    "            sample = dmatrix[sid], dmatrix[sid+1], dmatrix[sid+2], dmatrix[random.choice(indices)]\n",
    "        return sample, l\n",
    "    \n",
    "    while True:\n",
    "        # then prepare a batch of given size\n",
    "        C1, C2, C3, R, L = [], [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            xx, yy = generate_sample()\n",
    "            C1.append(xx[0])\n",
    "            C2.append(xx[1])\n",
    "            C3.append(xx[2])\n",
    "            R.append(xx[3])\n",
    "            L.append(yy)\n",
    "            \n",
    "        yield ([np.array(C1), np.array(C2), np.array(C3), np.array(R)], np.array(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aphex/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, model_from_json\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Lambda, Reshape, Flatten, Input, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from T_ops import *\n",
    "from commons import LossHistory, AUC_Saver, maybe_mkdir\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some re-usable building blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedder(embs_matrix, seqlen, vsiz=None, \n",
    "                   weighted=False, transformed=False, default_dim=300, prefix='word'):\n",
    "    \n",
    "    inp = Input(shape=(seqlen,))\n",
    "    \n",
    "    if embs_matrix is not None:\n",
    "        emb_dim = embs_matrix.shape[1]\n",
    "        voc_siz = embs_matrix.shape[0]\n",
    "        enc = Embedding(voc_siz, emb_dim, input_length=seqlen, \n",
    "                        weights=[embs_matrix], trainable = False)(inp)\n",
    "    else:\n",
    "        emb_dim = default_dim\n",
    "        voc_siz = vsiz\n",
    "        enc = Embedding(voc_siz, emb_dim, input_length=seqlen)(inp)\n",
    "        \n",
    "    if transformed:\n",
    "        trf = Dense(emb_dim)(enc)\n",
    "        act = LeakyReLU()(trf)\n",
    "    else:\n",
    "        act = enc\n",
    "    \n",
    "    if weighted:\n",
    "        wwt = Embedding(voc_siz, 1, input_length=seqlen,\n",
    "                        weights=[np.ones(shape=(voc_siz,1))])(inp)\n",
    "        wac = Reshape((-1,1))(Activation(\"softmax\")(Reshape((-1,))(wwt)))\n",
    "        out = Lambda(pairwise_mul, name='MulLayer')([act, wac])\n",
    "    else:\n",
    "        out = act\n",
    "    \n",
    "    return Model(inputs=[inp], outputs=out, name=prefix+'_embedding_model')\n",
    "\n",
    "def build_lstm_encoder(input_shape, return_sequences=False, bidirectional=False, lstm_dim=300, prefix=\"word\", rdp=0.1):\n",
    "    \n",
    "    LSTM_DIM = lstm_dim\n",
    "    inp = Input(shape=tuple(input_shape[-2:]))\n",
    "    if bidirectional:\n",
    "        rnn = Bidirectional(CuDNNLSTM(LSTM_DIM, return_sequences=return_sequences))(inp)\n",
    "    else:\n",
    "        rnn = CuDNNLSTM(LSTM_DIM, return_sequences=return_sequences)(inp)\n",
    "        \n",
    "    mod = Model(inputs=inp, outputs=rnn, name=prefix+'_lstm_encoder')\n",
    "    return mod\n",
    "\n",
    "def build_gru_encoder(input_shape, return_sequences=False, bidirectional=False, lstm_dim=300, prefix=\"word\", rdp=0.1):\n",
    "    \n",
    "    LSTM_DIM = lstm_dim\n",
    "    inp = Input(shape=tuple(input_shape[-2:]))\n",
    "    if bidirectional:\n",
    "        rnn = Bidirectional(CuDNNGRU(LSTM_DIM, return_sequences=return_sequences))(inp)\n",
    "    else:\n",
    "        rnn = CuDNNGRU(LSTM_DIM, return_sequences=return_sequences)(inp)\n",
    "        \n",
    "    mod = Model(inputs=inp, outputs=rnn, name=prefix+'_lstm_encoder')\n",
    "    return mod\n",
    "\n",
    "def build_deep_sim_net(input_shape, inr_dim=300, DROPOUT=0.3):\n",
    "    \n",
    "    input_a = Input(shape=(input_shape[-1],))\n",
    "    input_b = Input(shape=(input_shape[-1],))\n",
    "    \n",
    "    mul_layer = Lambda(pairwise_mul, name='MultiplicationLayer')([input_a, input_b])\n",
    "    dis_layer = Lambda(pairwise_dis, name='SubstractionLayer')([input_a, input_b])\n",
    "    css_layer = Lambda(cosine_similarity)([input_a, input_b])\n",
    "\n",
    "    dn1 = concatenate([mul_layer, dis_layer, css_layer])\n",
    "    bn1 = BatchNormalization()(dn1)\n",
    "    dr1 = Dropout(DROPOUT)(bn1)\n",
    "    \n",
    "    dn2 = Dense(inr_dim)(dr1)    \n",
    "    bn2 = BatchNormalization()(dn2)\n",
    "    ac2 = LeakyReLU(alpha=0.3)(bn2)\n",
    "        \n",
    "    mod = Model(inputs=[input_a, input_b], outputs=ac2, name='deep_sim_net')\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the main model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sd_memnet_model():\n",
    "\n",
    "    BDR=True\n",
    "    \n",
    "    DROPOUT=0.3\n",
    "    LSTM_DIM=300\n",
    "    SEQ_LEN = 20\n",
    "    DENSE_DIM = 600\n",
    "\n",
    "    # inputs for contexts and reply\n",
    "    inp_ctx1 = Input(shape=(SEQ_LEN,), name='inp_ctx_0')\n",
    "    inp_ctx2 = Input(shape=(SEQ_LEN,), name='inp_ctx_1')\n",
    "    inp_ctx3 = Input(shape=(SEQ_LEN,), name='inp_ctx_2')\n",
    "    inp_rpl = Input(shape=(SEQ_LEN,), name='inp_reply')\n",
    "\n",
    "    # word embedding model\n",
    "    embedder = build_embedder(myembs, SEQ_LEN)\n",
    "    \n",
    "    # shared sentence-level encoder\n",
    "    encoder_ctx = build_lstm_encoder(embedder.output_shape, return_sequences=False, \n",
    "                                     lstm_dim=LSTM_DIM, bidirectional=BDR, prefix=\"sentence\")\n",
    "\n",
    "    emb_ctx1 = embedder(inp_ctx1)\n",
    "    emb_ctx2 = embedder(inp_ctx2)\n",
    "    emb_ctx3 = embedder(inp_ctx3)\n",
    "    emb_rpl = embedder(inp_rpl)\n",
    "    \n",
    "    \n",
    "    def dense_comb():\n",
    "        # shared dense layer to combine context and reply vectors\n",
    "        inp1 = Input(shape=encoder_ctx.output_shape)\n",
    "        inp2 = Input(shape=encoder_ctx.output_shape)\n",
    "        \n",
    "        dens = Dense(DENSE_DIM, activation='relu')(concatenate([inp1, inp2]))\n",
    "        dens2 = Dense(DENSE_DIM, activation='relu')(dens)\n",
    "        return Model(inputs=[inp1, inp2], outputs=dens2, name='merge_model')\n",
    "    \n",
    "    dm = dense_comb()\n",
    "\n",
    "    # encode contexts and reply\n",
    "    enc_ctx1 = encoder_ctx(emb_ctx1)\n",
    "    enc_ctx2 = encoder_ctx(emb_ctx2)\n",
    "    enc_ctx3 = encoder_ctx(emb_ctx3)\n",
    "    enc_rpl = encoder_ctx(emb_rpl)\n",
    "    \n",
    "    # condition context encoding on reply encoding\n",
    "    ctx1_cmb = Reshape((1,DENSE_DIM))(dm([enc_ctx1, enc_rpl]))\n",
    "    ctx2_cmb = Reshape((1,DENSE_DIM))(dm([enc_ctx2, enc_rpl]))\n",
    "    ctx3_cmb = Reshape((1,DENSE_DIM))(dm([enc_ctx3, enc_rpl]))\n",
    "    \n",
    "    # encode the whole context into a single vector\n",
    "    ctx_h = CuDNNGRU(LSTM_DIM*2, return_sequences=False, name='context_lstm_encoder')(\n",
    "        concatenate([ctx1_cmb, ctx2_cmb, ctx3_cmb], axis=1))\n",
    "    \n",
    "    # 2-layer MLP to evaluate relatedness between context and reply\n",
    "    dsm = build_deep_sim_net((-1,LSTM_DIM*2), inr_dim=DENSE_DIM)\n",
    "    css = dsm([ctx_h, enc_rpl])\n",
    "\n",
    "    # output neuron (during pretraining we do binary classification)\n",
    "    fc2 = Dense(1, activation='sigmoid', name='relevance')(css)\n",
    "\n",
    "    model = Model(inputs=[inp_ctx1,inp_ctx2,inp_ctx3,inp_rpl], outputs=fc2)\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and prepare to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-16 14:22:43,370 : WARNING : From /home/aphex/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "model = get_sd_memnet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = 3\n",
    "class_name = 'memnet'\n",
    "net_name = 'lstm_rus_fasttext'\n",
    "\n",
    "model = get_sd_memnet_model()\n",
    "\n",
    "maybe_mkdir(\"./models/gen{}\".format(gen))\n",
    "maybe_mkdir(\"./models/gen{}/{}\".format(gen, class_name))\n",
    "\n",
    "chfilepath = \"./models/gen{}/{}/{}.hdf5\".format(gen, class_name, net_name)\n",
    "\n",
    "checkpointer = ModelCheckpoint(chfilepath, save_best_only=True)\n",
    "histlogger = LossHistory(\"./models/gen{}/{}/{}.csv\".format(gen, class_name, net_name))\n",
    "json.dump(model.to_json(), open(\"./models/gen{}/{}/{}.json\".format(gen, class_name, net_name), \"w\"))\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# adjust as needed \n",
    "traingen = generate_batch(VT[:7000000], batch_size=512)\n",
    "evalgen = generate_batch(VT[7000000:], batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches peak accuracy after 128-256 epochs and can be trained indefinetely without overfitting because of the sheer size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2048/2048 [==============================] - 184s 90ms/step - loss: 0.6495 - acc: 0.6088 - val_loss: 0.6422 - val_acc: 0.6221\n",
      "Epoch 2/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.6027 - acc: 0.6639 - val_loss: 0.6066 - val_acc: 0.6593\n",
      "Epoch 3/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5779 - acc: 0.6878 - val_loss: 0.5889 - val_acc: 0.6761\n",
      "Epoch 4/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5604 - acc: 0.7028 - val_loss: 0.5750 - val_acc: 0.6906\n",
      "Epoch 5/16\n",
      "2048/2048 [==============================] - 186s 91ms/step - loss: 0.5496 - acc: 0.7113 - val_loss: 0.5658 - val_acc: 0.6973\n",
      "Epoch 6/16\n",
      "2048/2048 [==============================] - 186s 91ms/step - loss: 0.5411 - acc: 0.7174 - val_loss: 0.5635 - val_acc: 0.6992\n",
      "Epoch 7/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5349 - acc: 0.7226 - val_loss: 0.5639 - val_acc: 0.7005\n",
      "Epoch 8/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5303 - acc: 0.7258 - val_loss: 0.5570 - val_acc: 0.7052\n",
      "Epoch 9/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5258 - acc: 0.7288 - val_loss: 0.5546 - val_acc: 0.7077\n",
      "Epoch 10/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5227 - acc: 0.7313 - val_loss: 0.5510 - val_acc: 0.7109\n",
      "Epoch 11/16\n",
      "2048/2048 [==============================] - 187s 92ms/step - loss: 0.5205 - acc: 0.7334 - val_loss: 0.5496 - val_acc: 0.7131\n",
      "Epoch 12/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5175 - acc: 0.7349 - val_loss: 0.5424 - val_acc: 0.7168\n",
      "Epoch 13/16\n",
      "2048/2048 [==============================] - 187s 92ms/step - loss: 0.5155 - acc: 0.7365 - val_loss: 0.5436 - val_acc: 0.7155\n",
      "Epoch 14/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5133 - acc: 0.7378 - val_loss: 0.5393 - val_acc: 0.7192\n",
      "Epoch 15/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5110 - acc: 0.7398 - val_loss: 0.5423 - val_acc: 0.7157\n",
      "Epoch 16/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5095 - acc: 0.7404 - val_loss: 0.5398 - val_acc: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81741f8240>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=traingen, steps_per_epoch=2048, epochs=16, \n",
    "                    validation_data=evalgen, validation_steps=256, callbacks=[checkpointer, histlogger, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_mpath = chfilepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fine-tune the pre-trained model on the high-quality competition data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads the architecture and weights of the pretrained-model, then modifies it for the task of the competition\n",
    "\n",
    "def make_pretrained_model(fpath):\n",
    "    model = model_from_json(json.load(open(fpath)))\n",
    "    model.load_weights(fpath.replace(\"json\", \"hdf5\"))\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(\"deep_sim_net\").get_output_at(1))\n",
    "    \n",
    "    # gets a list of model layers up to the MLP\n",
    "    lr = model.get_layer(\"deep_sim_net\").get_output_at(0)\n",
    "\n",
    "    # freezes all model weights except the MLP part which will be finetuned\n",
    "    for layer in intermediate_layer_model.layers[:-1]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    SEQ_LEN = 20\n",
    "\n",
    "    inp_ctx1 = Input(shape=(SEQ_LEN,))\n",
    "    inp_ctx2 = Input(shape=(SEQ_LEN,))\n",
    "    inp_ctx3 = Input(shape=(SEQ_LEN,))\n",
    "    inp_rpl = Input(shape=(SEQ_LEN,))\n",
    "\n",
    "    dns = intermediate_layer_model([inp_ctx1, inp_ctx2, inp_ctx3, inp_rpl])\n",
    "    \n",
    "    # during fine-tuning we do regression\n",
    "    dns_out = Dense(1)(dns)\n",
    "\n",
    "    fin_model = Model(inputs=[inp_ctx1, inp_ctx2, inp_ctx3, inp_rpl], outputs=dns_out)\n",
    "    fin_model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return fin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    columns = ['context_id','context_2','context_1','context_0','reply_id','reply','label','confidence']\n",
    "\n",
    "    test_df = pd.read_csv(\"./data/final.tsv\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "    train_df = pd.read_csv(\"./data/train.tsv\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    train_df = train_df.fillna(\"\")\n",
    "    test_df = test_df.fillna(\"\")\n",
    "\n",
    "    test_df.columns = columns[:-2]\n",
    "    train_df.columns = columns\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def tokenize_sents(sents, tokenizer):\n",
    "    return [tokenizer(s) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 481908.60it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 388859.63it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 357807.77it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 330870.01it/s]\n"
     ]
    }
   ],
   "source": [
    "cxv1 = vectorize(tokenize_sents(train_df['context_2'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "cxv2 = vectorize(tokenize_sents(train_df['context_1'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "cxv3 = vectorize(tokenize_sents(train_df['context_0'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "train_rpl_V = vectorize(tokenize_sents(train_df['reply'].tolist(), tokenizers.tokenize_split), voc, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104834/104834 [00:00<00:00, 490874.41it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 406835.40it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 359543.03it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 346161.97it/s]\n"
     ]
    }
   ],
   "source": [
    "ts_cxv1 = vectorize(tokenize_sents(test_df['context_2'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "ts_cxv2 = vectorize(tokenize_sents(test_df['context_1'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "ts_cxv3 = vectorize(tokenize_sents(test_df['context_0'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "ts_rpl_V = vectorize(tokenize_sents(test_df['reply'].tolist(), tokenizers.tokenize_split), voc, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = []\n",
    "for l in train_df['label'].tolist():\n",
    "    if l == 'good':\n",
    "        nl.append(1)\n",
    "    elif l == 'neutral':\n",
    "        nl.append(0.5)\n",
    "    elif l == 'bad':\n",
    "        nl.append(0)\n",
    "        \n",
    "y_train = np.array(nl)\n",
    "        \n",
    "confs = np.array(train_df['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Train on 87779 samples, validate on 9754 samples\n",
      "Epoch 1/16\n",
      "87779/87779 [==============================] - 8s 86us/step - loss: 0.2201 - acc: 0.5373 - val_loss: 0.1986 - val_acc: 0.5635\n",
      "Epoch 2/16\n",
      "87779/87779 [==============================] - 6s 70us/step - loss: 0.2006 - acc: 0.5662 - val_loss: 0.1973 - val_acc: 0.5691\n",
      "Epoch 3/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1995 - acc: 0.5696 - val_loss: 0.1960 - val_acc: 0.5751\n",
      "Epoch 4/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1989 - acc: 0.5707 - val_loss: 0.1972 - val_acc: 0.5744\n",
      "Epoch 5/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1978 - acc: 0.5736 - val_loss: 0.1970 - val_acc: 0.5698\n",
      "Epoch 6/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1979 - acc: 0.5729 - val_loss: 0.1947 - val_acc: 0.5743\n",
      "Epoch 7/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1977 - acc: 0.5735 - val_loss: 0.1947 - val_acc: 0.5747\n",
      "Epoch 8/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1973 - acc: 0.5747 - val_loss: 0.1947 - val_acc: 0.5737\n",
      "Epoch 9/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1971 - acc: 0.5757 - val_loss: 0.1946 - val_acc: 0.5785\n",
      "Epoch 10/16\n",
      "87779/87779 [==============================] - 7s 78us/step - loss: 0.1964 - acc: 0.5769 - val_loss: 0.1946 - val_acc: 0.5724\n",
      "Epoch 11/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1961 - acc: 0.5755 - val_loss: 0.1944 - val_acc: 0.5731\n",
      "Epoch 12/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1958 - acc: 0.5787 - val_loss: 0.1940 - val_acc: 0.5777\n",
      "Epoch 13/16\n",
      "87779/87779 [==============================] - 7s 79us/step - loss: 0.1953 - acc: 0.5787 - val_loss: 0.1939 - val_acc: 0.5742\n",
      "Epoch 14/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1949 - acc: 0.5792 - val_loss: 0.1938 - val_acc: 0.5754\n",
      "Epoch 15/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1946 - acc: 0.5810 - val_loss: 0.1927 - val_acc: 0.5803\n",
      "Epoch 16/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1943 - acc: 0.5820 - val_loss: 0.1934 - val_acc: 0.5738\n",
      "Processing fold 2\n",
      "Train on 87779 samples, validate on 9754 samples\n",
      "Epoch 1/16\n",
      "87779/87779 [==============================] - 8s 90us/step - loss: 0.2198 - acc: 0.5357 - val_loss: 0.2024 - val_acc: 0.5620\n",
      "Epoch 2/16\n",
      "87779/87779 [==============================] - 6s 72us/step - loss: 0.2002 - acc: 0.5664 - val_loss: 0.2004 - val_acc: 0.5678\n",
      "Epoch 3/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1990 - acc: 0.5696 - val_loss: 0.1994 - val_acc: 0.5694\n",
      "Epoch 4/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1983 - acc: 0.5710 - val_loss: 0.1991 - val_acc: 0.5694\n",
      "Epoch 5/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1978 - acc: 0.5736 - val_loss: 0.1985 - val_acc: 0.5710\n",
      "Epoch 6/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1972 - acc: 0.5742 - val_loss: 0.1982 - val_acc: 0.5723\n",
      "Epoch 7/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1975 - acc: 0.5735 - val_loss: 0.1992 - val_acc: 0.5716\n",
      "Epoch 8/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1967 - acc: 0.5750 - val_loss: 0.1976 - val_acc: 0.5753\n",
      "Epoch 9/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1959 - acc: 0.5772 - val_loss: 0.1975 - val_acc: 0.5753\n",
      "Epoch 10/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1959 - acc: 0.5772 - val_loss: 0.1988 - val_acc: 0.5721\n",
      "Epoch 11/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1957 - acc: 0.5760 - val_loss: 0.1980 - val_acc: 0.5751\n",
      "Epoch 12/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1953 - acc: 0.5791 - val_loss: 0.1974 - val_acc: 0.5768\n",
      "Epoch 13/16\n",
      "87779/87779 [==============================] - 7s 78us/step - loss: 0.1946 - acc: 0.5806 - val_loss: 0.1969 - val_acc: 0.5773\n",
      "Epoch 14/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1948 - acc: 0.5788 - val_loss: 0.1961 - val_acc: 0.5772\n",
      "Epoch 15/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1940 - acc: 0.5814 - val_loss: 0.1968 - val_acc: 0.5733\n",
      "Epoch 16/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1938 - acc: 0.5821 - val_loss: 0.1968 - val_acc: 0.5751\n",
      "Processing fold 3\n",
      "Train on 87779 samples, validate on 9754 samples\n",
      "Epoch 1/16\n",
      "87779/87779 [==============================] - 8s 93us/step - loss: 0.2166 - acc: 0.5395 - val_loss: 0.2016 - val_acc: 0.5607\n",
      "Epoch 2/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.2001 - acc: 0.5676 - val_loss: 0.1994 - val_acc: 0.5662\n",
      "Epoch 3/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1991 - acc: 0.5696 - val_loss: 0.1993 - val_acc: 0.5681\n",
      "Epoch 4/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1982 - acc: 0.5725 - val_loss: 0.1992 - val_acc: 0.5642\n",
      "Epoch 5/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1985 - acc: 0.5717 - val_loss: 0.1986 - val_acc: 0.5686\n",
      "Epoch 6/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1975 - acc: 0.5731 - val_loss: 0.1981 - val_acc: 0.5704\n",
      "Epoch 7/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1971 - acc: 0.5750 - val_loss: 0.1973 - val_acc: 0.5706\n",
      "Epoch 8/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1968 - acc: 0.5746 - val_loss: 0.1974 - val_acc: 0.5681\n",
      "Epoch 9/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1967 - acc: 0.5764 - val_loss: 0.1988 - val_acc: 0.5657\n",
      "Epoch 10/16\n",
      "87779/87779 [==============================] - 7s 78us/step - loss: 0.1964 - acc: 0.5763 - val_loss: 0.1977 - val_acc: 0.5755\n",
      "Epoch 11/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1958 - acc: 0.5784 - val_loss: 0.1965 - val_acc: 0.5748\n",
      "Epoch 12/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1951 - acc: 0.5796 - val_loss: 0.1977 - val_acc: 0.5736\n",
      "Epoch 13/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1949 - acc: 0.5796 - val_loss: 0.1967 - val_acc: 0.5740\n",
      "Epoch 14/16\n",
      "87779/87779 [==============================] - 7s 79us/step - loss: 0.1943 - acc: 0.5808 - val_loss: 0.1961 - val_acc: 0.5734\n",
      "Epoch 15/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1943 - acc: 0.5821 - val_loss: 0.1999 - val_acc: 0.5705\n",
      "Epoch 16/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1941 - acc: 0.5822 - val_loss: 0.1955 - val_acc: 0.5776\n",
      "Processing fold 4\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 8s 95us/step - loss: 0.2245 - acc: 0.5337 - val_loss: 0.1983 - val_acc: 0.5694\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.2001 - acc: 0.5668 - val_loss: 0.1960 - val_acc: 0.5796\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1997 - acc: 0.5689 - val_loss: 0.1971 - val_acc: 0.5750\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1986 - acc: 0.5716 - val_loss: 0.1946 - val_acc: 0.5764\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1982 - acc: 0.5724 - val_loss: 0.1959 - val_acc: 0.5739\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1981 - acc: 0.5717 - val_loss: 0.1962 - val_acc: 0.5767\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1975 - acc: 0.5736 - val_loss: 0.1941 - val_acc: 0.5772\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1969 - acc: 0.5763 - val_loss: 0.1937 - val_acc: 0.5799\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1968 - acc: 0.5762 - val_loss: 0.1938 - val_acc: 0.5770\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1965 - acc: 0.5749 - val_loss: 0.1940 - val_acc: 0.5779\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1961 - acc: 0.5774 - val_loss: 0.1937 - val_acc: 0.5828\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1959 - acc: 0.5778 - val_loss: 0.1936 - val_acc: 0.5835\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 80us/step - loss: 0.1956 - acc: 0.5795 - val_loss: 0.1924 - val_acc: 0.5831\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1954 - acc: 0.5775 - val_loss: 0.1929 - val_acc: 0.5829\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1950 - acc: 0.5796 - val_loss: 0.1918 - val_acc: 0.5839\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1947 - acc: 0.5807 - val_loss: 0.1927 - val_acc: 0.5824\n",
      "Processing fold 5\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 98us/step - loss: 0.2242 - acc: 0.5339 - val_loss: 0.2004 - val_acc: 0.5637\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.2007 - acc: 0.5654 - val_loss: 0.1973 - val_acc: 0.5699\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1994 - acc: 0.5684 - val_loss: 0.1970 - val_acc: 0.5695\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1988 - acc: 0.5707 - val_loss: 0.1961 - val_acc: 0.5722\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1984 - acc: 0.5716 - val_loss: 0.1958 - val_acc: 0.5748\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1978 - acc: 0.5740 - val_loss: 0.1957 - val_acc: 0.5719\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1975 - acc: 0.5752 - val_loss: 0.1953 - val_acc: 0.5760\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1970 - acc: 0.5748 - val_loss: 0.1950 - val_acc: 0.5749\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1969 - acc: 0.5760 - val_loss: 0.1957 - val_acc: 0.5744\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1966 - acc: 0.5751 - val_loss: 0.1950 - val_acc: 0.5796\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1961 - acc: 0.5776 - val_loss: 0.1952 - val_acc: 0.5751\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1960 - acc: 0.5779 - val_loss: 0.1943 - val_acc: 0.5789\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1958 - acc: 0.5782 - val_loss: 0.1946 - val_acc: 0.5782\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1953 - acc: 0.5782 - val_loss: 0.1944 - val_acc: 0.5794\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1948 - acc: 0.5806 - val_loss: 0.1936 - val_acc: 0.5793\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1949 - acc: 0.5801 - val_loss: 0.1934 - val_acc: 0.5778\n",
      "Processing fold 6\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 104us/step - loss: 0.2250 - acc: 0.5320 - val_loss: 0.2001 - val_acc: 0.5691\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.2004 - acc: 0.5657 - val_loss: 0.1982 - val_acc: 0.5791\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1991 - acc: 0.5677 - val_loss: 0.1964 - val_acc: 0.5811\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1988 - acc: 0.5704 - val_loss: 0.1962 - val_acc: 0.5803\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1982 - acc: 0.5709 - val_loss: 0.1969 - val_acc: 0.5829\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.1980 - acc: 0.5713 - val_loss: 0.1965 - val_acc: 0.5827\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1977 - acc: 0.5723 - val_loss: 0.1957 - val_acc: 0.5833\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1969 - acc: 0.5731 - val_loss: 0.1952 - val_acc: 0.5847\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1966 - acc: 0.5739 - val_loss: 0.1963 - val_acc: 0.5843\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1964 - acc: 0.5760 - val_loss: 0.1948 - val_acc: 0.5878\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1960 - acc: 0.5769 - val_loss: 0.1947 - val_acc: 0.5884\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1959 - acc: 0.5776 - val_loss: 0.1947 - val_acc: 0.5857\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1950 - acc: 0.5793 - val_loss: 0.1939 - val_acc: 0.5892\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1951 - acc: 0.5781 - val_loss: 0.1952 - val_acc: 0.5841\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1949 - acc: 0.5783 - val_loss: 0.1953 - val_acc: 0.5821\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1945 - acc: 0.5805 - val_loss: 0.1934 - val_acc: 0.5864\n",
      "Processing fold 7\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 102us/step - loss: 0.2203 - acc: 0.5348 - val_loss: 0.2002 - val_acc: 0.5724\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.2000 - acc: 0.5675 - val_loss: 0.1980 - val_acc: 0.5740\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1988 - acc: 0.5694 - val_loss: 0.1976 - val_acc: 0.5723\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1986 - acc: 0.5705 - val_loss: 0.1980 - val_acc: 0.5715\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1979 - acc: 0.5723 - val_loss: 0.1963 - val_acc: 0.5753\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1974 - acc: 0.5730 - val_loss: 0.1963 - val_acc: 0.5753\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1969 - acc: 0.5751 - val_loss: 0.1961 - val_acc: 0.5773\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1967 - acc: 0.5751 - val_loss: 0.1958 - val_acc: 0.5778\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1965 - acc: 0.5767 - val_loss: 0.1964 - val_acc: 0.5739\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1966 - acc: 0.5757 - val_loss: 0.1960 - val_acc: 0.5736\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1961 - acc: 0.5774 - val_loss: 0.1971 - val_acc: 0.5757\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1957 - acc: 0.5784 - val_loss: 0.1955 - val_acc: 0.5776\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1952 - acc: 0.5786 - val_loss: 0.1948 - val_acc: 0.5814\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1947 - acc: 0.5797 - val_loss: 0.1946 - val_acc: 0.5819\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1946 - acc: 0.5810 - val_loss: 0.1947 - val_acc: 0.5800\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1944 - acc: 0.5808 - val_loss: 0.1950 - val_acc: 0.5761\n",
      "Processing fold 8\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 102us/step - loss: 0.2205 - acc: 0.5348 - val_loss: 0.1987 - val_acc: 0.5809\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.2003 - acc: 0.5655 - val_loss: 0.1971 - val_acc: 0.5796\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1990 - acc: 0.5683 - val_loss: 0.1967 - val_acc: 0.5816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1985 - acc: 0.5709 - val_loss: 0.1953 - val_acc: 0.5828\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1985 - acc: 0.5703 - val_loss: 0.1951 - val_acc: 0.5849\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 7s 80us/step - loss: 0.1978 - acc: 0.5712 - val_loss: 0.1949 - val_acc: 0.5855\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 81us/step - loss: 0.1974 - acc: 0.5717 - val_loss: 0.1951 - val_acc: 0.5868\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1973 - acc: 0.5733 - val_loss: 0.1953 - val_acc: 0.5830\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1962 - acc: 0.5755 - val_loss: 0.1944 - val_acc: 0.5866\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1961 - acc: 0.5753 - val_loss: 0.1942 - val_acc: 0.5886\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1961 - acc: 0.5758 - val_loss: 0.1941 - val_acc: 0.5855\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1961 - acc: 0.5757 - val_loss: 0.1949 - val_acc: 0.5845\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1954 - acc: 0.5777 - val_loss: 0.1938 - val_acc: 0.5918\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1949 - acc: 0.5800 - val_loss: 0.1944 - val_acc: 0.5838\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1949 - acc: 0.5799 - val_loss: 0.1931 - val_acc: 0.5894\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1943 - acc: 0.5806 - val_loss: 0.1932 - val_acc: 0.5879\n",
      "Processing fold 9\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 101us/step - loss: 0.2302 - acc: 0.5321 - val_loss: 0.2041 - val_acc: 0.5600\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1996 - acc: 0.5688 - val_loss: 0.2036 - val_acc: 0.5649\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1987 - acc: 0.5698 - val_loss: 0.2012 - val_acc: 0.5635\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1980 - acc: 0.5726 - val_loss: 0.2008 - val_acc: 0.5653\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1977 - acc: 0.5727 - val_loss: 0.2004 - val_acc: 0.5710\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1974 - acc: 0.5731 - val_loss: 0.2004 - val_acc: 0.5654\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1968 - acc: 0.5747 - val_loss: 0.1998 - val_acc: 0.5721\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 80us/step - loss: 0.1967 - acc: 0.5759 - val_loss: 0.1993 - val_acc: 0.5753\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1964 - acc: 0.5771 - val_loss: 0.1993 - val_acc: 0.5739\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1961 - acc: 0.5766 - val_loss: 0.1996 - val_acc: 0.5743\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1958 - acc: 0.5783 - val_loss: 0.2001 - val_acc: 0.5722\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1953 - acc: 0.5800 - val_loss: 0.1989 - val_acc: 0.5731\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1954 - acc: 0.5788 - val_loss: 0.1992 - val_acc: 0.5730\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1949 - acc: 0.5785 - val_loss: 0.1998 - val_acc: 0.5720\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1943 - acc: 0.5806 - val_loss: 0.1985 - val_acc: 0.5742\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1940 - acc: 0.5819 - val_loss: 0.1985 - val_acc: 0.5728\n",
      "Processing fold 10\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 101us/step - loss: 0.2263 - acc: 0.5333 - val_loss: 0.1988 - val_acc: 0.5644\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.2004 - acc: 0.5666 - val_loss: 0.1968 - val_acc: 0.5712\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1992 - acc: 0.5705 - val_loss: 0.1962 - val_acc: 0.5710\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1985 - acc: 0.5712 - val_loss: 0.1963 - val_acc: 0.5740\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1980 - acc: 0.5729 - val_loss: 0.1956 - val_acc: 0.5736\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1978 - acc: 0.5744 - val_loss: 0.1963 - val_acc: 0.5734\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1973 - acc: 0.5750 - val_loss: 0.1952 - val_acc: 0.5750\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.1970 - acc: 0.5761 - val_loss: 0.1949 - val_acc: 0.5760\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1968 - acc: 0.5770 - val_loss: 0.1945 - val_acc: 0.5761\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1964 - acc: 0.5784 - val_loss: 0.1940 - val_acc: 0.5761\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1958 - acc: 0.5778 - val_loss: 0.1942 - val_acc: 0.5731\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1956 - acc: 0.5792 - val_loss: 0.1941 - val_acc: 0.5785\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1955 - acc: 0.5788 - val_loss: 0.1943 - val_acc: 0.5790\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1949 - acc: 0.5808 - val_loss: 0.1934 - val_acc: 0.5804\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1944 - acc: 0.5810 - val_loss: 0.1930 - val_acc: 0.5815\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1939 - acc: 0.5834 - val_loss: 0.1939 - val_acc: 0.5811\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=34)\n",
    "\n",
    "splits = []\n",
    "oofpreds = []\n",
    "tspreds = []\n",
    "train = True\n",
    "\n",
    "\n",
    "for i, split in enumerate(kf.split(list(range(cxv1.shape[0])))):\n",
    "    splits.append(split)\n",
    "    print(\"Processing fold {}\".format(i+1))\n",
    "    \n",
    "    model = make_pretrained_model(pretrained_mpath.replace(\"hdf5\", \"json\"))\n",
    "    \n",
    "    tX = [cxv1[split[0]], cxv2[split[0]], cxv3[split[0]], train_rpl_V[split[0]]]\n",
    "    #tX = [train_ctx_V[split[0]], train_rpl_V[split[0]]]\n",
    "    tY = y_train[split[0]]\n",
    "    \n",
    "    vX = [cxv1[split[1]], cxv2[split[1]], cxv3[split[1]], train_rpl_V[split[1]]]\n",
    "    #vX = [train_ctx_V[split[1]], train_rpl_V[split[1]]]\n",
    "    vY = y_train[split[1]]\n",
    "    \n",
    "    gen = 3\n",
    "    class_name = 'pretrained'\n",
    "    net_name = 'pretrained_lstm_rus_fasttext_'+str(i)\n",
    "    \n",
    "    maybe_mkdir(\"./models/gen{}\".format(gen))\n",
    "    maybe_mkdir(\"./models/gen{}/{}\".format(gen, class_name))\n",
    "    \n",
    "    chfilepath = \"./models/gen{}/{}/{}.hdf5\".format(gen, class_name, net_name)\n",
    "    if train:\n",
    "        checkpointer = ModelCheckpoint(chfilepath, save_best_only=True)\n",
    "        histlogger = LossHistory(\"./models/gen{}/{}/{}.csv\".format(gen, class_name, net_name))\n",
    "        json.dump(model.to_json(), open(\"./models/gen{}/{}/{}.json\".format(gen, class_name, net_name), \"w\"))\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                      patience=5, min_lr=0.0001)\n",
    "\n",
    "        h = model.fit(tX, tY, validation_data=(vX, vY), \n",
    "                     batch_size=512, epochs=16,\n",
    "                     verbose=1, callbacks=[checkpointer, histlogger, reduce_lr])\n",
    "    \n",
    "    model.load_weights(chfilepath)\n",
    "    \n",
    "    oofp = model.predict(vX, batch_size=512)\n",
    "\n",
    "    oofpreds.append([oofp, split[1]])\n",
    "    tspreds.append(model.predict([ts_cxv1, ts_cxv2, ts_cxv3, ts_rpl_V], batch_size=512))\n",
    "    #tspreds.append(model.predict([ts_ctx_V, ts_rpl_V], batch_size=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, save the vocabs for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([voc, rvoc], \n",
    "            open(\"./assets/rus_fasttext.voc\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

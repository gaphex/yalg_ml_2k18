{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "import seaborn\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "\n",
    "from gensim_w2v import tokenizers\n",
    "from text_processing_utils import vectorize, build_vocab, get_embeddings, read_fasttext\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains and describes the procedures necessary for pre-training a deep NN on OPUS data and fine-tuning it on the competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-training we will be using the OPUS dataset, freely available for download at http://opus.nlpl.eu/OpenSubtitles2018.php. \n",
    "\n",
    "The repository contains a small sample of the data, consider downloading the whole dataset to get the real results. Russian dataset can be downloaded at http://opus.nlpl.eu/download.php?f=OpenSubtitles2016/en-ru.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read OPUS data\n",
    "# this might as well be english or spanish subtitle data\n",
    "\n",
    "tokens = []\n",
    "c = 0\n",
    "with open(\"./assets/cleaned_subs.txt\", \"r\") as fi:\n",
    "    for l in fi:\n",
    "        tokens.append(l.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough we will use a pre-trained russian fasttext model (not included).\n",
    "\n",
    "You can get one at https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.ru.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read russian fasttext model\n",
    "w2v = read_fasttext(\"./assets/cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n"
     ]
    }
   ],
   "source": [
    "# prepare word-to-id and id-to-word mappings\n",
    "voc, rvoc = build_vocab(tokens, 250000, emb_model=w2v)\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NULL': 0,\n",
       " 'UNKN': 1,\n",
       " '<S>': 2,\n",
       " '</S>': 3,\n",
       " '.': 4,\n",
       " ',': 5,\n",
       " 'я': 6,\n",
       " '?': 7,\n",
       " '-': 8,\n",
       " 'не': 9,\n",
       " 'что': 10,\n",
       " 'и': 11,\n",
       " 'в': 12,\n",
       " 'это': 13,\n",
       " 'ты': 14,\n",
       " '!': 15,\n",
       " '...': 16,\n",
       " 'на': 17,\n",
       " 'с': 18,\n",
       " 'он': 19,\n",
       " 'мы': 20,\n",
       " 'как': 21,\n",
       " 'вы': 22,\n",
       " 'у': 23,\n",
       " 'но': 24,\n",
       " 'мне': 25,\n",
       " 'меня': 26,\n",
       " 'а': 27,\n",
       " 'так': 28,\n",
       " 'его': 29,\n",
       " 'она': 30,\n",
       " 'да': 31,\n",
       " 'нет': 32,\n",
       " 'все': 33,\n",
       " 'если': 34,\n",
       " 'о': 35,\n",
       " 'тебя': 36,\n",
       " 'бы': 37,\n",
       " 'за': 38,\n",
       " 'они': 39,\n",
       " 'тебе': 40,\n",
       " 'чтобы': 41,\n",
       " 'из': 42,\n",
       " 'к': 43,\n",
       " 'же': 44,\n",
       " 'когда': 45,\n",
       " 'для': 46,\n",
       " 'есть': 47,\n",
       " 'просто': 48,\n",
       " 'был': 49,\n",
       " 'по': 50,\n",
       " 'то': 51,\n",
       " 'только': 52,\n",
       " 'было': 53,\n",
       " 'ну': 54,\n",
       " 'здесь': 55,\n",
       " 'может': 56,\n",
       " 'от': 57,\n",
       " 'нас': 58,\n",
       " 'вас': 59,\n",
       " 'быть': 60,\n",
       " 'знаю': 61,\n",
       " 'всё': 62,\n",
       " 'их': 63,\n",
       " 'будет': 64,\n",
       " 'или': 65,\n",
       " 'вам': 66,\n",
       " 'вот': 67,\n",
       " 'кто': 68,\n",
       " 'уже': 69,\n",
       " 'еще': 70,\n",
       " 'нам': 71,\n",
       " 'почему': 72,\n",
       " 'очень': 73,\n",
       " 'была': 74,\n",
       " 'ее': 75,\n",
       " 'этого': 76,\n",
       " 'там': 77,\n",
       " 'могу': 78,\n",
       " 'чем': 79,\n",
       " 'хорошо': 80,\n",
       " 'хочу': 81,\n",
       " 'нужно': 82,\n",
       " 'сейчас': 83,\n",
       " 'потому': 84,\n",
       " 'этом': 85,\n",
       " 'до': 86,\n",
       " 'мой': 87,\n",
       " 'где': 88,\n",
       " 'ничего': 89,\n",
       " 'знаешь': 90,\n",
       " 'теперь': 91,\n",
       " 'больше': 92,\n",
       " 'её': 93,\n",
       " 'ему': 94,\n",
       " 'думаю': 95,\n",
       " 'время': 96,\n",
       " ':': 97,\n",
       " 'этот': 98,\n",
       " 'со': 99,\n",
       " 'даже': 100,\n",
       " 'раз': 101,\n",
       " 'того': 102,\n",
       " 'ли': 103,\n",
       " 'себя': 104,\n",
       " 'сказал': 105,\n",
       " 'были': 106,\n",
       " 'должен': 107,\n",
       " 'него': 108,\n",
       " 'никогда': 109,\n",
       " 'надо': 110,\n",
       " 'ни': 111,\n",
       " 'пока': 112,\n",
       " 'ещё': 113,\n",
       " 'что-то': 114,\n",
       " '—': 115,\n",
       " 'об': 116,\n",
       " 'тогда': 117,\n",
       " 'тут': 118,\n",
       " 'ладно': 119,\n",
       " 'тоже': 120,\n",
       " 'моя': 121,\n",
       " 'хочешь': 122,\n",
       " 'один': 123,\n",
       " 'сказать': 124,\n",
       " 'спасибо': 125,\n",
       " 'всегда': 126,\n",
       " 'том': 127,\n",
       " 'ей': 128,\n",
       " 'сделать': 129,\n",
       " 'сегодня': 130,\n",
       " 'лучше': 131,\n",
       " 'должны': 132,\n",
       " 'эти': 133,\n",
       " 'можешь': 134,\n",
       " 'без': 135,\n",
       " 'значит': 136,\n",
       " 'дело': 137,\n",
       " 'день': 138,\n",
       " 'тобой': 139,\n",
       " 'мной': 140,\n",
       " 'после': 141,\n",
       " 'во': 142,\n",
       " 'много': 143,\n",
       " 'лет': 144,\n",
       " 'них': 145,\n",
       " 'можно': 146,\n",
       " 'всего': 147,\n",
       " 'этим': 148,\n",
       " 'правда': 149,\n",
       " 'хотел': 150,\n",
       " '–': 151,\n",
       " 'конечно': 152,\n",
       " 'который': 153,\n",
       " 'жизнь': 154,\n",
       " 'буду': 155,\n",
       " 'эй': 156,\n",
       " 'давай': 157,\n",
       " 'потом': 158,\n",
       " 'всех': 159,\n",
       " 'сюда': 160,\n",
       " 'ним': 161,\n",
       " 'мистер': 162,\n",
       " 'им': 163,\n",
       " 'делать': 164,\n",
       " 'такой': 165,\n",
       " 'люди': 166,\n",
       " 'твой': 167,\n",
       " 'чего': 168,\n",
       " 'через': 169,\n",
       " 'пожалуйста': 170,\n",
       " 'ней': 171,\n",
       " 'этой': 172,\n",
       " 'себе': 173,\n",
       " 'слишком': 174,\n",
       " 'ведь': 175,\n",
       " 'свою': 176,\n",
       " 'привет': 177,\n",
       " 'моей': 178,\n",
       " 'возможно': 179,\n",
       " 'под': 180,\n",
       " 'знаете': 181,\n",
       " 'человек': 182,\n",
       " 'которые': 183,\n",
       " 'эту': 184,\n",
       " 'мог': 185,\n",
       " 'немного': 186,\n",
       " 'назад': 187,\n",
       " 'должна': 188,\n",
       " 'из-за': 189,\n",
       " 'такое': 190,\n",
       " 'жизни': 191,\n",
       " 'людей': 192,\n",
       " 'думаешь': 193,\n",
       " 'эта': 194,\n",
       " 'никто': 195,\n",
       " 'можем': 196,\n",
       " 'мои': 197,\n",
       " 'сказала': 198,\n",
       " 'тот': 199,\n",
       " 'друг': 200,\n",
       " 'вместе': 201,\n",
       " 'сэр': 202,\n",
       " 'твоя': 203,\n",
       " 'сделал': 204,\n",
       " 'зачем': 205,\n",
       " 'отец': 206,\n",
       " 'поэтому': 207,\n",
       " 'кажется': 208,\n",
       " 'два': 209,\n",
       " 'сколько': 210,\n",
       " 'место': 211,\n",
       " 'хочет': 212,\n",
       " 'снова': 213,\n",
       " 'деньги': 214,\n",
       " 'видел': 215,\n",
       " 'парень': 216,\n",
       " 'кто-то': 217,\n",
       " 'всем': 218,\n",
       " 'своей': 219,\n",
       " 'знать': 220,\n",
       " 'тем': 221,\n",
       " 'времени': 222,\n",
       " '♪': 223,\n",
       " 'лишь': 224,\n",
       " 'похоже': 225,\n",
       " 'найти': 226,\n",
       " 'думал': 227,\n",
       " 'говорил': 228,\n",
       " 'точно': 229,\n",
       " 'нравится': 230,\n",
       " 'помочь': 231,\n",
       " 'мама': 232,\n",
       " 'куда': 233,\n",
       " 'дома': 234,\n",
       " 'стоит': 235,\n",
       " 'прямо': 236,\n",
       " 'именно': 237,\n",
       " 'будем': 238,\n",
       " 'можете': 239,\n",
       " ')': 240,\n",
       " 'сам': 241,\n",
       " 'моего': 242,\n",
       " 'знал': 243,\n",
       " 'свои': 244,\n",
       " 'доктор': 245,\n",
       " 'несколько': 246,\n",
       " 'деле': 247,\n",
       " 'говорит': 248,\n",
       " 'знает': 249,\n",
       " 'домой': 250,\n",
       " '(': 251,\n",
       " 'порядке': 252,\n",
       " 'понимаю': 253,\n",
       " 'говорить': 254,\n",
       " 'разве': 255,\n",
       " 'хотела': 256,\n",
       " 'тех': 257,\n",
       " 'три': 258,\n",
       " 'слушай': 259,\n",
       " 'дом': 260,\n",
       " 'должно': 261,\n",
       " 'действительно': 262,\n",
       " 'каждый': 263,\n",
       " 'случилось': 264,\n",
       " 'какой': 265,\n",
       " 'ваш': 266,\n",
       " 'что-нибудь': 267,\n",
       " 'будешь': 268,\n",
       " 'совсем': 269,\n",
       " 'хотите': 270,\n",
       " 'перед': 271,\n",
       " 'про': 272,\n",
       " 'наш': 273,\n",
       " 'вами': 274,\n",
       " 'люблю': 275,\n",
       " 'одна': 276,\n",
       " '*': 277,\n",
       " 'нее': 278,\n",
       " 'будут': 279,\n",
       " 'свой': 280,\n",
       " 'более': 281,\n",
       " 'оно': 282,\n",
       " 'самом': 283,\n",
       " 'поговорить': 284,\n",
       " 'мою': 285,\n",
       " 'туда': 286,\n",
       " 'боже': 287,\n",
       " 'кого': 288,\n",
       " 'пор': 289,\n",
       " 'нужен': 290,\n",
       " 'имя': 291,\n",
       " 'скажи': 292,\n",
       " 'работу': 293,\n",
       " 'при': 294,\n",
       " 'говорю': 295,\n",
       " 'против': 296,\n",
       " 'твоей': 297,\n",
       " 'дела': 298,\n",
       " 'этих': 299,\n",
       " 'вообще': 300,\n",
       " 'ваша': 301,\n",
       " 'уверен': 302,\n",
       " 'нужна': 303,\n",
       " 'всю': 304,\n",
       " 'давайте': 305,\n",
       " 'ж': 306,\n",
       " 'могли': 307,\n",
       " 'раньше': 308,\n",
       " 'уж': 309,\n",
       " 'могут': 310,\n",
       " 'твои': 311,\n",
       " 'над': 312,\n",
       " 'наши': 313,\n",
       " 'итак': 314,\n",
       " 'ради': 315,\n",
       " 'мать': 316,\n",
       " 'отсюда': 317,\n",
       " 'завтра': 318,\n",
       " 'прошу': 319,\n",
       " 'которая': 320,\n",
       " 'хотя': 321,\n",
       " 'нами': 322,\n",
       " 'будто': 323,\n",
       " 'своего': 324,\n",
       " 'другой': 325,\n",
       " 'между': 326,\n",
       " 'надеюсь': 327,\n",
       " 'года': 328,\n",
       " 'этому': 329,\n",
       " 'человека': 330,\n",
       " 'кое-что': 331,\n",
       " 'наверное': 332,\n",
       " 'нельзя': 333,\n",
       " 'мисс': 334,\n",
       " 'такая': 335,\n",
       " 'ко': 336,\n",
       " '..': 337,\n",
       " 'ага': 338,\n",
       " 'такие': 339,\n",
       " 'ребята': 340,\n",
       " 'иди': 341,\n",
       " 'происходит': 342,\n",
       " 'могла': 343,\n",
       " 'вижу': 344,\n",
       " 'равно': 345,\n",
       " 'откуда': 346,\n",
       " 'вещи': 347,\n",
       " 'вопрос': 348,\n",
       " 'весь': 349,\n",
       " 'виду': 350,\n",
       " 'папа': 351,\n",
       " 'почти': 352,\n",
       " 'своих': 353,\n",
       " 'убить': 354,\n",
       " 'неё': 355,\n",
       " 'жить': 356,\n",
       " 'видеть': 357,\n",
       " 'делаешь': 358,\n",
       " 'понял': 359,\n",
       " 'хоть': 360,\n",
       " 'ночь': 361,\n",
       " 'достаточно': 362,\n",
       " 'дня': 363,\n",
       " 'думала': 364,\n",
       " 'прости': 365,\n",
       " 'знаем': 366,\n",
       " 'дай': 367,\n",
       " 'смерти': 368,\n",
       " 'понимаешь': 369,\n",
       " 'пару': 370,\n",
       " 'детей': 371,\n",
       " 'ваши': 372,\n",
       " 'которую': 373,\n",
       " 'самое': 374,\n",
       " 'отлично': 375,\n",
       " 'верно': 376,\n",
       " 'минут': 377,\n",
       " 'первый': 378,\n",
       " 'хватит': 379,\n",
       " 'жаль': 380,\n",
       " 'чём': 381,\n",
       " 'мир': 382,\n",
       " 'собой': 383,\n",
       " 'черт': 384,\n",
       " 'послушай': 385,\n",
       " 'нашли': 386,\n",
       " 'простите': 387,\n",
       " 'работа': 388,\n",
       " 'таким': 389,\n",
       " 'одного': 390,\n",
       " 'сказали': 391,\n",
       " 'ваше': 392,\n",
       " 'зовут': 393,\n",
       " 'иногда': 394,\n",
       " 'скажу': 395,\n",
       " 'слышал': 396,\n",
       " 'имею': 397,\n",
       " 'никаких': 398,\n",
       " 'скоро': 399,\n",
       " 'мое': 400,\n",
       " 'отца': 401,\n",
       " 'руки': 402,\n",
       " 'нашей': 403,\n",
       " 'миссис': 404,\n",
       " 'насчет': 405,\n",
       " 'пусть': 406,\n",
       " 'кроме': 407,\n",
       " 'также': 408,\n",
       " 'месте': 409,\n",
       " 'твою': 410,\n",
       " 'говорят': 411,\n",
       " 'одно': 412,\n",
       " 'случае': 413,\n",
       " 'вроде': 414,\n",
       " 'сын': 415,\n",
       " 'тому': 416,\n",
       " 'видела': 417,\n",
       " 'говоришь': 418,\n",
       " 'денег': 419,\n",
       " 'такого': 420,\n",
       " 'делает': 421,\n",
       " 'идти': 422,\n",
       " 'смогу': 423,\n",
       " 'убил': 424,\n",
       " 'рядом': 425,\n",
       " 'вашей': 426,\n",
       " 'какая': 427,\n",
       " 'моих': 428,\n",
       " 'те': 429,\n",
       " 'друга': 430,\n",
       " 'быстро': 431,\n",
       " 'кем': 432,\n",
       " 'знала': 433,\n",
       " 'женщина': 434,\n",
       " 'плохо': 435,\n",
       " 'говорила': 436,\n",
       " 'нужны': 437,\n",
       " '[': 438,\n",
       " 'возьми': 439,\n",
       " ']': 440,\n",
       " '€': 441,\n",
       " '10': 442,\n",
       " 'своим': 443,\n",
       " 'одной': 444,\n",
       " 'работать': 445,\n",
       " 'собираюсь': 446,\n",
       " 'вчера': 447,\n",
       " 'увидеть': 448,\n",
       " 'сделала': 449,\n",
       " 'проблемы': 450,\n",
       " 'проблема': 451,\n",
       " 'две': 452,\n",
       " 'произошло': 453,\n",
       " 'последний': 454,\n",
       " 'брат': 455,\n",
       " 'долго': 456,\n",
       " 'твоего': 457,\n",
       " 'ясно': 458,\n",
       " 'пять': 459,\n",
       " 'видишь': 460,\n",
       " 'работает': 461,\n",
       " 'часть': 462,\n",
       " 'ночью': 463,\n",
       " 'одну': 464,\n",
       " 'думаете': 465,\n",
       " 'моим': 466,\n",
       " 'сама': 467,\n",
       " 'номер': 468,\n",
       " 'стал': 469,\n",
       " 'ними': 470,\n",
       " 'вообще-то': 471,\n",
       " '#': 472,\n",
       " 'узнать': 473,\n",
       " 'прав': 474,\n",
       " 'жена': 475,\n",
       " 'дети': 476,\n",
       " 'пойти': 477,\n",
       " 'затем': 478,\n",
       " 'наша': 479,\n",
       " 'парня': 480,\n",
       " 'придется': 481,\n",
       " 'дальше': 482,\n",
       " 'которой': 483,\n",
       " 'стать': 484,\n",
       " 'машину': 485,\n",
       " 'слова': 486,\n",
       " 'вечером': 487,\n",
       " 'будь': 488,\n",
       " 'которого': 489,\n",
       " 'готов': 490,\n",
       " 'важно': 491,\n",
       " 'год': 492,\n",
       " 'имеет': 493,\n",
       " 'момент': 494,\n",
       " 'сердце': 495,\n",
       " 'глаза': 496,\n",
       " 'вернуться': 497,\n",
       " 'большой': 498,\n",
       " 'чувак': 499,\n",
       " 'i': 500,\n",
       " 'сначала': 501,\n",
       " 'рад': 502,\n",
       " 'утром': 503,\n",
       " 'the': 504,\n",
       " 'сразу': 505,\n",
       " 'посмотри': 506,\n",
       " 'часов': 507,\n",
       " 'пора': 508,\n",
       " 'чувствую': 509,\n",
       " 'давно': 510,\n",
       " 'видели': 511,\n",
       " 'правильно': 512,\n",
       " '--': 513,\n",
       " 'хороший': 514,\n",
       " 'сделали': 515,\n",
       " 'смотри': 516,\n",
       " 'взять': 517,\n",
       " 'той': 518,\n",
       " 'однажды': 519,\n",
       " 'работы': 520,\n",
       " 'дверь': 521,\n",
       " 'помощь': 522,\n",
       " 'дней': 523,\n",
       " 'понять': 524,\n",
       " 'сильно': 525,\n",
       " 'прежде': 526,\n",
       " 'идея': 527,\n",
       " 'you': 528,\n",
       " 'права': 529,\n",
       " 'шанс': 530,\n",
       " 'опять': 531,\n",
       " 'любовь': 532,\n",
       " 'доме': 533,\n",
       " 'голову': 534,\n",
       " 'наших': 535,\n",
       " 'уверена': 536,\n",
       " 'дать': 537,\n",
       " 'новый': 538,\n",
       " 'друзья': 539,\n",
       " 'оба': 540,\n",
       " 'помню': 541,\n",
       " 'дорогая': 542,\n",
       " 'пойду': 543,\n",
       " 'получить': 544,\n",
       " 'нашел': 545,\n",
       " 'самый': 546,\n",
       " 'столько': 547,\n",
       " 'свое': 548,\n",
       " 'довольно': 549,\n",
       " 'вашего': 550,\n",
       " 'слово': 551,\n",
       " 'мире': 552,\n",
       " 'помнишь': 553,\n",
       " 'парни': 554,\n",
       " 'дочь': 555,\n",
       " 'боюсь': 556,\n",
       " 'скорее': 557,\n",
       " 'твое': 558,\n",
       " 'пошли': 559,\n",
       " '2': 560,\n",
       " '20': 561,\n",
       " 'другие': 562,\n",
       " 'смысле': 563,\n",
       " 'девушка': 564,\n",
       " 'тело': 565,\n",
       " 'чтоб': 566,\n",
       " 'извините': 567,\n",
       " 'будете': 568,\n",
       " 'которое': 569,\n",
       " 'кто-нибудь': 570,\n",
       " 'обычно': 571,\n",
       " 'уйти': 572,\n",
       " 'нашего': 573,\n",
       " 'обо': 574,\n",
       " 'которых': 575,\n",
       " 'рассказать': 576,\n",
       " 'какие': 577,\n",
       " 'пришел': 578,\n",
       " 'та': 579,\n",
       " 'полагаю': 580,\n",
       " 'город': 581,\n",
       " 'насколько': 582,\n",
       " 'других': 583,\n",
       " 'некоторые': 584,\n",
       " 'хотят': 585,\n",
       " 'бог': 586,\n",
       " 'честь': 587,\n",
       " 'лицо': 588,\n",
       " 'хотели': 589,\n",
       " 'пытался': 590,\n",
       " 'нормально': 591,\n",
       " 'нем': 592,\n",
       " 'извини': 593,\n",
       " 'умер': 594,\n",
       " 'никого': 595,\n",
       " 'ума': 596,\n",
       " 'иначе': 597,\n",
       " 'другом': 598,\n",
       " 'думать': 599,\n",
       " 'вся': 600,\n",
       " 'какой-то': 601,\n",
       " 'любит': 602,\n",
       " 'вечер': 603,\n",
       " 'обратно': 604,\n",
       " 'получил': 605,\n",
       " 'пути': 606,\n",
       " 'говорите': 607,\n",
       " 'делал': 608,\n",
       " 'подумал': 609,\n",
       " 'кого-то': 610,\n",
       " 'выглядит': 611,\n",
       " 'места': 612,\n",
       " 'поверить': 613,\n",
       " 'господи': 614,\n",
       " 'дал': 615,\n",
       " 'говорили': 616,\n",
       " 'кому': 617,\n",
       " 'интересно': 618,\n",
       " 'единственный': 619,\n",
       " 'решил': 620,\n",
       " '5': 621,\n",
       " 'ночи': 622,\n",
       " 'пришлось': 623,\n",
       " 'типа': 624,\n",
       " 'способ': 625,\n",
       " 'моё': 626,\n",
       " 'таких': 627,\n",
       " 'план': 628,\n",
       " 'правду': 629,\n",
       " 'хуже': 630,\n",
       " 'капитан': 631,\n",
       " 'семья': 632,\n",
       " 'здорово': 633,\n",
       " 'безопасности': 634,\n",
       " 'подожди': 635,\n",
       " 'совершенно': 636,\n",
       " 'посмотреть': 637,\n",
       " 'хорошая': 638,\n",
       " 'час': 639,\n",
       " '-я': 640,\n",
       " 'a': 641,\n",
       " 'кстати': 642,\n",
       " 'году': 643,\n",
       " 'сделаю': 644,\n",
       " 'кровь': 645,\n",
       " 'двух': 646,\n",
       " 'ох': 647,\n",
       " 'сможешь': 648,\n",
       " 'стороны': 649,\n",
       " 'вокруг': 650,\n",
       " 'мере': 651,\n",
       " 'ребенка': 652,\n",
       " 'когда-нибудь': 653,\n",
       " 'говори': 654,\n",
       " 'долларов': 655,\n",
       " '-да': 656,\n",
       " 'работе': 657,\n",
       " '3': 658,\n",
       " 'какого': 659,\n",
       " 'часа': 660,\n",
       " 'смог': 661,\n",
       " 'сами': 662,\n",
       " 'сможем': 663,\n",
       " 'смотреть': 664,\n",
       " 'четыре': 665,\n",
       " 'начала': 666,\n",
       " 'где-то': 667,\n",
       " 'идет': 668,\n",
       " 'знали': 669,\n",
       " 'серьезно': 670,\n",
       " 'пришли': 671,\n",
       " 'муж': 672,\n",
       " 'понимаете': 673,\n",
       " 'неделю': 674,\n",
       " 'взял': 675,\n",
       " 'делаю': 676,\n",
       " 'крови': 677,\n",
       " 'дорогой': 678,\n",
       " 'играть': 679,\n",
       " 'прекрасно': 680,\n",
       " 'оружие': 681,\n",
       " 'ах': 682,\n",
       " 'внутри': 683,\n",
       " 'сына': 684,\n",
       " 'женщины': 685,\n",
       " 'около': 686,\n",
       " 'сестра': 687,\n",
       " 'станет': 688,\n",
       " 'вашу': 689,\n",
       " 'как-то': 690,\n",
       " 'месяцев': 691,\n",
       " 'милая': 692,\n",
       " 'машина': 693,\n",
       " 'угодно': 694,\n",
       " 'вернулся': 695,\n",
       " 'ту': 696,\n",
       " 'утро': 697,\n",
       " 'семьи': 698,\n",
       " 'далеко': 699,\n",
       " 'смерть': 700,\n",
       " 'господин': 701,\n",
       " 'остаться': 702,\n",
       " 'сих': 703,\n",
       " 'джон': 704,\n",
       " 'путь': 705,\n",
       " 'отношения': 706,\n",
       " 'сможет': 707,\n",
       " 'хотим': 708,\n",
       " 'следующий': 709,\n",
       " 'конце': 710,\n",
       " 'пришла': 711,\n",
       " 'осталось': 712,\n",
       " 'мужчина': 713,\n",
       " 'нашу': 714,\n",
       " 'пытаюсь': 715,\n",
       " 'ждать': 716,\n",
       " 'ребенок': 717,\n",
       " 'нашла': 718,\n",
       " 'какое': 719,\n",
       " 'история': 720,\n",
       " 'твоих': 721,\n",
       " 'странно': 722,\n",
       " 'знают': 723,\n",
       " 'постоянно': 724,\n",
       " 'говоря': 725,\n",
       " 'спать': 726,\n",
       " 'конец': 727,\n",
       " 'решение': 728,\n",
       " 'звучит': 729,\n",
       " 'вдруг': 730,\n",
       " 'честно': 731,\n",
       " 'телефон': 732,\n",
       " 'следует': 733,\n",
       " 'образом': 734,\n",
       " 'вернуть': 735,\n",
       " 'руку': 736,\n",
       " 'скажите': 737,\n",
       " 'поздно': 738,\n",
       " 'двое': 739,\n",
       " 'начать': 740,\n",
       " 'внимание': 741,\n",
       " 'машины': 742,\n",
       " 'иметь': 743,\n",
       " 'например': 744,\n",
       " 'полиции': 745,\n",
       " 'второй': 746,\n",
       " 'леди': 747,\n",
       " 'шоу': 748,\n",
       " 'полностью': 749,\n",
       " 'верю': 750,\n",
       " 'использовать': 751,\n",
       " 'матери': 752,\n",
       " 'скажешь': 753,\n",
       " 'часто': 754,\n",
       " 'наше': 755,\n",
       " 'людям': 756,\n",
       " 'посмотрим': 757,\n",
       " 'меньше': 758,\n",
       " 'лучший': 759,\n",
       " 'никому': 760,\n",
       " 'убийство': 761,\n",
       " 'новости': 762,\n",
       " 'спасти': 763,\n",
       " 'твоим': 764,\n",
       " 'чёрт': 765,\n",
       " 'нему': 766,\n",
       " 'крайней': 767,\n",
       " 'настолько': 768,\n",
       " 'кофе': 769,\n",
       " 'to': 770,\n",
       " 'убили': 771,\n",
       " 'недели': 772,\n",
       " 'мальчик': 773,\n",
       " 'последние': 774,\n",
       " 'прошлой': 775,\n",
       " 'видите': 776,\n",
       " 'трудно': 777,\n",
       " 'приятно': 778,\n",
       " 'всей': 779,\n",
       " 'никакого': 780,\n",
       " 'вместо': 781,\n",
       " 'другого': 782,\n",
       " 'комнате': 783,\n",
       " 'либо': 784,\n",
       " 'слушайте': 785,\n",
       " 'ужасно': 786,\n",
       " 'истории': 787,\n",
       " 'думает': 788,\n",
       " 'вон': 789,\n",
       " 'своими': 790,\n",
       " 'имени': 791,\n",
       " 'ой': 792,\n",
       " 'узнал': 793,\n",
       " 'полиция': 794,\n",
       " 'самого': 795,\n",
       " 'шесть': 796,\n",
       " 'слышала': 797,\n",
       " '4': 798,\n",
       " 'случай': 799,\n",
       " 'стало': 800,\n",
       " 'школе': 801,\n",
       " 'поможет': 802,\n",
       " 'друзей': 803,\n",
       " 'городе': 804,\n",
       " 'ваших': 805,\n",
       " 'послушайте': 806,\n",
       " 'очевидно': 807,\n",
       " 'господа': 808,\n",
       " 'понятно': 809,\n",
       " 'возможность': 810,\n",
       " 'другое': 811,\n",
       " 'готовы': 812,\n",
       " 'дайте': 813,\n",
       " 'особенно': 814,\n",
       " 'рада': 815,\n",
       " 'оставить': 816,\n",
       " 'любой': 817,\n",
       " 'всему': 818,\n",
       " 'быстрее': 819,\n",
       " 'чуть': 820,\n",
       " 'легко': 821,\n",
       " 'агент': 822,\n",
       " 'пойдем': 823,\n",
       " 'машине': 824,\n",
       " 'убийца': 825,\n",
       " 'спросить': 826,\n",
       " 'человеком': 827,\n",
       " 'означает': 828,\n",
       " 'приятель': 829,\n",
       " 'свет': 830,\n",
       " 'тысяч': 831,\n",
       " 'маленький': 832,\n",
       " 'чему': 833,\n",
       " 'известно': 834,\n",
       " 'убийства': 835,\n",
       " 'большая': 836,\n",
       " 'позже': 837,\n",
       " 'родители': 838,\n",
       " '%': 839,\n",
       " 'забыл': 840,\n",
       " 'делают': 841,\n",
       " 'стала': 842,\n",
       " 'чарли': 843,\n",
       " 'мало': 844,\n",
       " 'моем': 845,\n",
       " 'последнее': 846,\n",
       " 'джо': 847,\n",
       " 'минуту': 848,\n",
       " 'которым': 849,\n",
       " 'ненавижу': 850,\n",
       " 'собираешься': 851,\n",
       " 'любви': 852,\n",
       " 'города': 853,\n",
       " 'вопросы': 854,\n",
       " 'начал': 855,\n",
       " 'рождения': 856,\n",
       " 'пошел': 857,\n",
       " 'показать': 858,\n",
       " '$': 859,\n",
       " 'любишь': 860,\n",
       " 'ответ': 861,\n",
       " 'купить': 862,\n",
       " 'наконец': 863,\n",
       " 'представить': 864,\n",
       " 'остановить': 865,\n",
       " 'увидел': 866,\n",
       " 'дерьмо': 867,\n",
       " 'прошлом': 868,\n",
       " 'работал': 869,\n",
       " 'вина': 870,\n",
       " 'выбор': 871,\n",
       " 'поняла': 872,\n",
       " 'мадам': 873,\n",
       " 'мира': 874,\n",
       " 'мам': 875,\n",
       " 'школу': 876,\n",
       " 'историю': 877,\n",
       " '15': 878,\n",
       " 'девочка': 879,\n",
       " 'бывает': 880,\n",
       " 'речь': 881,\n",
       " 'неужели': 882,\n",
       " 'посмотрите': 883,\n",
       " 'джек': 884,\n",
       " 'потерял': 885,\n",
       " 'здравствуйте': 886,\n",
       " 'своё': 887,\n",
       " 'среди': 888,\n",
       " 'причина': 889,\n",
       " 'выйти': 890,\n",
       " 'настоящий': 891,\n",
       " 'боль': 892,\n",
       " '6': 893,\n",
       " 'любом': 894,\n",
       " 'одним': 895,\n",
       " 'поводу': 896,\n",
       " 'насчёт': 897,\n",
       " 'вещь': 898,\n",
       " 'пол': 899,\n",
       " '30': 900,\n",
       " 'готова': 901,\n",
       " 'женщин': 902,\n",
       " 'смотрите': 903,\n",
       " 'таком': 904,\n",
       " 'оставил': 905,\n",
       " 'право': 906,\n",
       " 'позволить': 907,\n",
       " 'хотелось': 908,\n",
       " 'придётся': 909,\n",
       " 'компании': 910,\n",
       " 'забрать': 911,\n",
       " 'проблем': 912,\n",
       " 'милый': 913,\n",
       " 'словно': 914,\n",
       " 'невозможно': 915,\n",
       " 'сэм': 916,\n",
       " 'ввиду': 917,\n",
       " 'сделаем': 918,\n",
       " 'десять': 919,\n",
       " 'вести': 920,\n",
       " 'неделе': 921,\n",
       " 'людьми': 922,\n",
       " 'ноги': 923,\n",
       " 'хорошие': 924,\n",
       " 'детка': 925,\n",
       " 'самая': 926,\n",
       " 'бога': 927,\n",
       " 'принять': 928,\n",
       " 'абсолютно': 929,\n",
       " 'понятия': 930,\n",
       " 'месяц': 931,\n",
       " 'искать': 932,\n",
       " 'б': 933,\n",
       " 'большое': 934,\n",
       " 'повезло': 935,\n",
       " 'сообщение': 936,\n",
       " 'другим': 937,\n",
       " 'связи': 938,\n",
       " 'мистера': 939,\n",
       " 'моему': 940,\n",
       " 'чтo': 941,\n",
       " 'мужик': 942,\n",
       " 'вероятно': 943,\n",
       " 'конца': 944,\n",
       " 'вернусь': 945,\n",
       " 'делаете': 946,\n",
       " 'твоё': 947,\n",
       " 'умереть': 948,\n",
       " 'по-моему': 949,\n",
       " 'мужа': 950,\n",
       " 'больно': 951,\n",
       " 'доброе': 952,\n",
       " 'добро': 953,\n",
       " ';': 954,\n",
       " 'умерла': 955,\n",
       " 'подумать': 956,\n",
       " 'котором': 957,\n",
       " 'детектив': 958,\n",
       " 'стороне': 959,\n",
       " 'работаю': 960,\n",
       " 'добрый': 961,\n",
       " '12': 962,\n",
       " 'предложение': 963,\n",
       " 'является': 964,\n",
       " 'позвонить': 965,\n",
       " 'счет': 966,\n",
       " 'держать': 967,\n",
       " '50': 968,\n",
       " 'вперед': 969,\n",
       " 'брата': 970,\n",
       " 'отцом': 971,\n",
       " 'месяца': 972,\n",
       " 'улице': 973,\n",
       " 'сложно': 974,\n",
       " 'своем': 975,\n",
       " 'рассказал': 976,\n",
       " 'стали': 977,\n",
       " 'воды': 978,\n",
       " 'намного': 979,\n",
       " 'многие': 980,\n",
       " 'игры': 981,\n",
       " 'ужин': 982,\n",
       " 'обязательно': 983,\n",
       " 'фильм': 984,\n",
       " 'прошло': 985,\n",
       " 'босс': 986,\n",
       " 'имеешь': 987,\n",
       " 'находится': 988,\n",
       " 'корабль': 989,\n",
       " 'президент': 990,\n",
       " 'связь': 991,\n",
       " 'подумала': 992,\n",
       " 'смешно': 993,\n",
       " 'пистолет': 994,\n",
       " 'майкл': 995,\n",
       " 'доктора': 996,\n",
       " 'список': 997,\n",
       " 'никакой': 998,\n",
       " 'комнату': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "myembs = get_embeddings(w2v, rvoc)\n",
    "myembs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9016819/9016819 [00:36<00:00, 245561.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# represent data as a matrix of indices \n",
    "VT = vectorize(tokens, voc, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch generator\n",
    "\n",
    "def generate_batch(dmatrix, batch_size=256):\n",
    "\n",
    "    indices = np.arange(0, len(dmatrix)-4)\n",
    "    \n",
    "    def generate_sample():\n",
    "        sid = random.choice(indices)\n",
    "        l = random.choice([0,1])\n",
    "        if l:\n",
    "            # we either use 4 consecutive utterances (positive sample)\n",
    "            sample = dmatrix[sid], dmatrix[sid+1], dmatrix[sid+2], dmatrix[sid+3]\n",
    "        else:\n",
    "            # or 3 consecutive and one random utterance (negative sample)\n",
    "            sample = dmatrix[sid], dmatrix[sid+1], dmatrix[sid+2], dmatrix[random.choice(indices)]\n",
    "        return sample, l\n",
    "    \n",
    "    while True:\n",
    "        # then prepare a batch of given size\n",
    "        C1, C2, C3, R, L = [], [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            xx, yy = generate_sample()\n",
    "            C1.append(xx[0])\n",
    "            C2.append(xx[1])\n",
    "            C3.append(xx[2])\n",
    "            R.append(xx[3])\n",
    "            L.append(yy)\n",
    "            \n",
    "        yield ([np.array(C1), np.array(C2), np.array(C3), np.array(R)], np.array(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aphex/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, model_from_json\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Lambda, Reshape, Flatten, Input, CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from T_ops import *\n",
    "from commons import LossHistory, AUC_Saver, maybe_mkdir\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some re-usable building blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedder(embs_matrix, seqlen, vsiz=None, \n",
    "                   weighted=False, transformed=False, default_dim=300, prefix='word'):\n",
    "    \n",
    "    inp = Input(shape=(seqlen,))\n",
    "    \n",
    "    if embs_matrix is not None:\n",
    "        emb_dim = embs_matrix.shape[1]\n",
    "        voc_siz = embs_matrix.shape[0]\n",
    "        enc = Embedding(voc_siz, emb_dim, input_length=seqlen, \n",
    "                        weights=[embs_matrix], trainable = False)(inp)\n",
    "    else:\n",
    "        emb_dim = default_dim\n",
    "        voc_siz = vsiz\n",
    "        enc = Embedding(voc_siz, emb_dim, input_length=seqlen)(inp)\n",
    "        \n",
    "    if transformed:\n",
    "        trf = Dense(emb_dim)(enc)\n",
    "        act = LeakyReLU()(trf)\n",
    "    else:\n",
    "        act = enc\n",
    "    \n",
    "    if weighted:\n",
    "        wwt = Embedding(voc_siz, 1, input_length=seqlen,\n",
    "                        weights=[np.ones(shape=(voc_siz,1))])(inp)\n",
    "        wac = Reshape((-1,1))(Activation(\"softmax\")(Reshape((-1,))(wwt)))\n",
    "        out = Lambda(pairwise_mul, name='MulLayer')([act, wac])\n",
    "    else:\n",
    "        out = act\n",
    "    \n",
    "    return Model(inputs=[inp], outputs=out, name=prefix+'_embedding_model')\n",
    "\n",
    "def build_lstm_encoder(input_shape, return_sequences=False, bidirectional=False, lstm_dim=300, prefix=\"word\", rdp=0.1):\n",
    "    \n",
    "    LSTM_DIM = lstm_dim\n",
    "    inp = Input(shape=tuple(input_shape[-2:]))\n",
    "    if bidirectional:\n",
    "        rnn = Bidirectional(CuDNNLSTM(LSTM_DIM, return_sequences=return_sequences))(inp)\n",
    "    else:\n",
    "        rnn = CuDNNLSTM(LSTM_DIM, return_sequences=return_sequences)(inp)\n",
    "        \n",
    "    mod = Model(inputs=inp, outputs=rnn, name=prefix+'_lstm_encoder')\n",
    "    return mod\n",
    "\n",
    "def build_gru_encoder(input_shape, return_sequences=False, bidirectional=False, lstm_dim=300, prefix=\"word\", rdp=0.1):\n",
    "    \n",
    "    LSTM_DIM = lstm_dim\n",
    "    inp = Input(shape=tuple(input_shape[-2:]))\n",
    "    if bidirectional:\n",
    "        rnn = Bidirectional(CuDNNGRU(LSTM_DIM, return_sequences=return_sequences))(inp)\n",
    "    else:\n",
    "        rnn = CuDNNGRU(LSTM_DIM, return_sequences=return_sequences)(inp)\n",
    "        \n",
    "    mod = Model(inputs=inp, outputs=rnn, name=prefix+'_lstm_encoder')\n",
    "    return mod\n",
    "\n",
    "def build_deep_sim_net(input_shape, inr_dim=300, DROPOUT=0.3):\n",
    "    \n",
    "    input_a = Input(shape=(input_shape[-1],))\n",
    "    input_b = Input(shape=(input_shape[-1],))\n",
    "    \n",
    "    mul_layer = Lambda(pairwise_mul, name='MultiplicationLayer')([input_a, input_b])\n",
    "    dis_layer = Lambda(pairwise_dis, name='SubstractionLayer')([input_a, input_b])\n",
    "    css_layer = Lambda(cosine_similarity)([input_a, input_b])\n",
    "\n",
    "    dn1 = concatenate([mul_layer, dis_layer, css_layer])\n",
    "    bn1 = BatchNormalization()(dn1)\n",
    "    dr1 = Dropout(DROPOUT)(bn1)\n",
    "    \n",
    "    dn2 = Dense(inr_dim)(dr1)    \n",
    "    bn2 = BatchNormalization()(dn2)\n",
    "    ac2 = LeakyReLU(alpha=0.3)(bn2)\n",
    "        \n",
    "    mod = Model(inputs=[input_a, input_b], outputs=ac2, name='deep_sim_net')\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the main model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sd_memnet_model():\n",
    "\n",
    "    BDR=True\n",
    "    \n",
    "    DROPOUT=0.3\n",
    "    LSTM_DIM=300\n",
    "    SEQ_LEN = 20\n",
    "    DENSE_DIM = 600\n",
    "\n",
    "    # inputs for contexts and reply\n",
    "    inp_ctx1 = Input(shape=(SEQ_LEN,), name='inp_ctx_0')\n",
    "    inp_ctx2 = Input(shape=(SEQ_LEN,), name='inp_ctx_1')\n",
    "    inp_ctx3 = Input(shape=(SEQ_LEN,), name='inp_ctx_2')\n",
    "    inp_rpl = Input(shape=(SEQ_LEN,), name='inp_reply')\n",
    "\n",
    "    # word embedding model\n",
    "    embedder = build_embedder(myembs, SEQ_LEN)\n",
    "    \n",
    "    # shared sentence-level encoder\n",
    "    encoder_ctx = build_lstm_encoder(embedder.output_shape, return_sequences=False, \n",
    "                                     lstm_dim=LSTM_DIM, bidirectional=BDR, prefix=\"sentence\")\n",
    "\n",
    "    emb_ctx1 = embedder(inp_ctx1)\n",
    "    emb_ctx2 = embedder(inp_ctx2)\n",
    "    emb_ctx3 = embedder(inp_ctx3)\n",
    "    emb_rpl = embedder(inp_rpl)\n",
    "    \n",
    "    \n",
    "    def dense_comb():\n",
    "        # shared dense layer to combine context and reply vectors\n",
    "        inp1 = Input(shape=encoder_ctx.output_shape)\n",
    "        inp2 = Input(shape=encoder_ctx.output_shape)\n",
    "        \n",
    "        dens = Dense(DENSE_DIM, activation='relu')(concatenate([inp1, inp2]))\n",
    "        dens2 = Dense(DENSE_DIM, activation='relu')(dens)\n",
    "        return Model(inputs=[inp1, inp2], outputs=dens2, name='merge_model')\n",
    "    \n",
    "    dm = dense_comb()\n",
    "\n",
    "    # encode contexts and reply\n",
    "    enc_ctx1 = encoder_ctx(emb_ctx1)\n",
    "    enc_ctx2 = encoder_ctx(emb_ctx2)\n",
    "    enc_ctx3 = encoder_ctx(emb_ctx3)\n",
    "    enc_rpl = encoder_ctx(emb_rpl)\n",
    "    \n",
    "    # condition context encoding on reply encoding\n",
    "    ctx1_cmb = Reshape((1,DENSE_DIM))(dm([enc_ctx1, enc_rpl]))\n",
    "    ctx2_cmb = Reshape((1,DENSE_DIM))(dm([enc_ctx2, enc_rpl]))\n",
    "    ctx3_cmb = Reshape((1,DENSE_DIM))(dm([enc_ctx3, enc_rpl]))\n",
    "    \n",
    "    # encode the whole context into a single vector\n",
    "    ctx_h = CuDNNGRU(LSTM_DIM*2, return_sequences=False, name='context_lstm_encoder')(\n",
    "        concatenate([ctx1_cmb, ctx2_cmb, ctx3_cmb], axis=1))\n",
    "    \n",
    "    # 2-layer MLP to evaluate relatedness between context and reply\n",
    "    dsm = build_deep_sim_net((-1,LSTM_DIM*2), inr_dim=DENSE_DIM)\n",
    "    css = dsm([ctx_h, enc_rpl])\n",
    "\n",
    "    # output neuron (during pretraining we do binary classification)\n",
    "    fc2 = Dense(1, activation='sigmoid', name='relevance')(css)\n",
    "\n",
    "    model = Model(inputs=[inp_ctx1,inp_ctx2,inp_ctx3,inp_rpl], outputs=fc2)\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and prepare to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-16 14:22:43,370 : WARNING : From /home/aphex/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "model = get_sd_memnet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = 3\n",
    "class_name = 'memnet'\n",
    "net_name = 'lstm_rus_fasttext'\n",
    "\n",
    "model = get_sd_memnet_model()\n",
    "\n",
    "maybe_mkdir(\"./models/gen{}\".format(gen))\n",
    "maybe_mkdir(\"./models/gen{}/{}\".format(gen, class_name))\n",
    "\n",
    "chfilepath = \"./models/gen{}/{}/{}.hdf5\".format(gen, class_name, net_name)\n",
    "\n",
    "checkpointer = ModelCheckpoint(chfilepath, save_best_only=True)\n",
    "histlogger = LossHistory(\"./models/gen{}/{}/{}.csv\".format(gen, class_name, net_name))\n",
    "json.dump(model.to_json(), open(\"./models/gen{}/{}/{}.json\".format(gen, class_name, net_name), \"w\"))\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# adjust as needed \n",
    "traingen = generate_batch(VT[:7000000], batch_size=512)\n",
    "evalgen = generate_batch(VT[7000000:], batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches peak accuracy after 128-256 epochs and can be trained indefinetely without overfitting because of the sheer size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2048/2048 [==============================] - 184s 90ms/step - loss: 0.6495 - acc: 0.6088 - val_loss: 0.6422 - val_acc: 0.6221\n",
      "Epoch 2/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.6027 - acc: 0.6639 - val_loss: 0.6066 - val_acc: 0.6593\n",
      "Epoch 3/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5779 - acc: 0.6878 - val_loss: 0.5889 - val_acc: 0.6761\n",
      "Epoch 4/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5604 - acc: 0.7028 - val_loss: 0.5750 - val_acc: 0.6906\n",
      "Epoch 5/16\n",
      "2048/2048 [==============================] - 186s 91ms/step - loss: 0.5496 - acc: 0.7113 - val_loss: 0.5658 - val_acc: 0.6973\n",
      "Epoch 6/16\n",
      "2048/2048 [==============================] - 186s 91ms/step - loss: 0.5411 - acc: 0.7174 - val_loss: 0.5635 - val_acc: 0.6992\n",
      "Epoch 7/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5349 - acc: 0.7226 - val_loss: 0.5639 - val_acc: 0.7005\n",
      "Epoch 8/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5303 - acc: 0.7258 - val_loss: 0.5570 - val_acc: 0.7052\n",
      "Epoch 9/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5258 - acc: 0.7288 - val_loss: 0.5546 - val_acc: 0.7077\n",
      "Epoch 10/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5227 - acc: 0.7313 - val_loss: 0.5510 - val_acc: 0.7109\n",
      "Epoch 11/16\n",
      "2048/2048 [==============================] - 187s 92ms/step - loss: 0.5205 - acc: 0.7334 - val_loss: 0.5496 - val_acc: 0.7131\n",
      "Epoch 12/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5175 - acc: 0.7349 - val_loss: 0.5424 - val_acc: 0.7168\n",
      "Epoch 13/16\n",
      "2048/2048 [==============================] - 187s 92ms/step - loss: 0.5155 - acc: 0.7365 - val_loss: 0.5436 - val_acc: 0.7155\n",
      "Epoch 14/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5133 - acc: 0.7378 - val_loss: 0.5393 - val_acc: 0.7192\n",
      "Epoch 15/16\n",
      "2048/2048 [==============================] - 188s 92ms/step - loss: 0.5110 - acc: 0.7398 - val_loss: 0.5423 - val_acc: 0.7157\n",
      "Epoch 16/16\n",
      "2048/2048 [==============================] - 187s 91ms/step - loss: 0.5095 - acc: 0.7404 - val_loss: 0.5398 - val_acc: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81741f8240>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=traingen, steps_per_epoch=2048, epochs=16, \n",
    "                    validation_data=evalgen, validation_steps=256, callbacks=[checkpointer, histlogger, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_mpath = chfilepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fine-tune the pre-trained model on the high-quality competition data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads the architecture and weights of the pretrained-model, then modifies it for the task of the competition\n",
    "\n",
    "def make_pretrained_model(fpath):\n",
    "    model = model_from_json(json.load(open(fpath)))\n",
    "    model.load_weights(fpath.replace(\"json\", \"hdf5\"))\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(\"deep_sim_net\").get_output_at(1))\n",
    "    \n",
    "    # gets a list of model layers up to the MLP\n",
    "    lr = model.get_layer(\"deep_sim_net\").get_output_at(0)\n",
    "\n",
    "    # freezes all model weights except the MLP part which will be finetuned\n",
    "    for layer in intermediate_layer_model.layers[:-1]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    SEQ_LEN = 20\n",
    "\n",
    "    inp_ctx1 = Input(shape=(SEQ_LEN,))\n",
    "    inp_ctx2 = Input(shape=(SEQ_LEN,))\n",
    "    inp_ctx3 = Input(shape=(SEQ_LEN,))\n",
    "    inp_rpl = Input(shape=(SEQ_LEN,))\n",
    "\n",
    "    dns = intermediate_layer_model([inp_ctx1, inp_ctx2, inp_ctx3, inp_rpl])\n",
    "    \n",
    "    # during fine-tuning we do regression\n",
    "    dns_out = Dense(1)(dns)\n",
    "\n",
    "    fin_model = Model(inputs=[inp_ctx1, inp_ctx2, inp_ctx3, inp_rpl], outputs=dns_out)\n",
    "    fin_model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return fin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    columns = ['context_id','context_2','context_1','context_0','reply_id','reply','label','confidence']\n",
    "\n",
    "    test_df = pd.read_csv(\"./data/final.tsv\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "    train_df = pd.read_csv(\"./data/train.tsv\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    train_df = train_df.fillna(\"\")\n",
    "    test_df = test_df.fillna(\"\")\n",
    "\n",
    "    test_df.columns = columns[:-2]\n",
    "    train_df.columns = columns\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def tokenize_sents(sents, tokenizer):\n",
    "    return [tokenizer(s) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 481908.60it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 388859.63it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 357807.77it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 330870.01it/s]\n"
     ]
    }
   ],
   "source": [
    "cxv1 = vectorize(tokenize_sents(train_df['context_2'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "cxv2 = vectorize(tokenize_sents(train_df['context_1'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "cxv3 = vectorize(tokenize_sents(train_df['context_0'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "train_rpl_V = vectorize(tokenize_sents(train_df['reply'].tolist(), tokenizers.tokenize_split), voc, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104834/104834 [00:00<00:00, 490874.41it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 406835.40it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 359543.03it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 346161.97it/s]\n"
     ]
    }
   ],
   "source": [
    "ts_cxv1 = vectorize(tokenize_sents(test_df['context_2'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "ts_cxv2 = vectorize(tokenize_sents(test_df['context_1'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "ts_cxv3 = vectorize(tokenize_sents(test_df['context_0'].tolist(), tokenizers.tokenize_split), voc, max_len=20)\n",
    "ts_rpl_V = vectorize(tokenize_sents(test_df['reply'].tolist(), tokenizers.tokenize_split), voc, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = []\n",
    "for l in train_df['label'].tolist():\n",
    "    if l == 'good':\n",
    "        nl.append(1)\n",
    "    elif l == 'neutral':\n",
    "        nl.append(0.5)\n",
    "    elif l == 'bad':\n",
    "        nl.append(0)\n",
    "        \n",
    "y_train = np.array(nl)\n",
    "        \n",
    "confs = np.array(train_df['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Train on 87779 samples, validate on 9754 samples\n",
      "Epoch 1/16\n",
      "87779/87779 [==============================] - 8s 86us/step - loss: 0.2201 - acc: 0.5373 - val_loss: 0.1986 - val_acc: 0.5635\n",
      "Epoch 2/16\n",
      "87779/87779 [==============================] - 6s 70us/step - loss: 0.2006 - acc: 0.5662 - val_loss: 0.1973 - val_acc: 0.5691\n",
      "Epoch 3/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1995 - acc: 0.5696 - val_loss: 0.1960 - val_acc: 0.5751\n",
      "Epoch 4/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1989 - acc: 0.5707 - val_loss: 0.1972 - val_acc: 0.5744\n",
      "Epoch 5/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1978 - acc: 0.5736 - val_loss: 0.1970 - val_acc: 0.5698\n",
      "Epoch 6/16\n",
      "87779/87779 [==============================] - 6s 71us/step - loss: 0.1979 - acc: 0.5729 - val_loss: 0.1947 - val_acc: 0.5743\n",
      "Epoch 7/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1977 - acc: 0.5735 - val_loss: 0.1947 - val_acc: 0.5747\n",
      "Epoch 8/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1973 - acc: 0.5747 - val_loss: 0.1947 - val_acc: 0.5737\n",
      "Epoch 9/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1971 - acc: 0.5757 - val_loss: 0.1946 - val_acc: 0.5785\n",
      "Epoch 10/16\n",
      "87779/87779 [==============================] - 7s 78us/step - loss: 0.1964 - acc: 0.5769 - val_loss: 0.1946 - val_acc: 0.5724\n",
      "Epoch 11/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1961 - acc: 0.5755 - val_loss: 0.1944 - val_acc: 0.5731\n",
      "Epoch 12/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1958 - acc: 0.5787 - val_loss: 0.1940 - val_acc: 0.5777\n",
      "Epoch 13/16\n",
      "87779/87779 [==============================] - 7s 79us/step - loss: 0.1953 - acc: 0.5787 - val_loss: 0.1939 - val_acc: 0.5742\n",
      "Epoch 14/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1949 - acc: 0.5792 - val_loss: 0.1938 - val_acc: 0.5754\n",
      "Epoch 15/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1946 - acc: 0.5810 - val_loss: 0.1927 - val_acc: 0.5803\n",
      "Epoch 16/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1943 - acc: 0.5820 - val_loss: 0.1934 - val_acc: 0.5738\n",
      "Processing fold 2\n",
      "Train on 87779 samples, validate on 9754 samples\n",
      "Epoch 1/16\n",
      "87779/87779 [==============================] - 8s 90us/step - loss: 0.2198 - acc: 0.5357 - val_loss: 0.2024 - val_acc: 0.5620\n",
      "Epoch 2/16\n",
      "87779/87779 [==============================] - 6s 72us/step - loss: 0.2002 - acc: 0.5664 - val_loss: 0.2004 - val_acc: 0.5678\n",
      "Epoch 3/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1990 - acc: 0.5696 - val_loss: 0.1994 - val_acc: 0.5694\n",
      "Epoch 4/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1983 - acc: 0.5710 - val_loss: 0.1991 - val_acc: 0.5694\n",
      "Epoch 5/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1978 - acc: 0.5736 - val_loss: 0.1985 - val_acc: 0.5710\n",
      "Epoch 6/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1972 - acc: 0.5742 - val_loss: 0.1982 - val_acc: 0.5723\n",
      "Epoch 7/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1975 - acc: 0.5735 - val_loss: 0.1992 - val_acc: 0.5716\n",
      "Epoch 8/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1967 - acc: 0.5750 - val_loss: 0.1976 - val_acc: 0.5753\n",
      "Epoch 9/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1959 - acc: 0.5772 - val_loss: 0.1975 - val_acc: 0.5753\n",
      "Epoch 10/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1959 - acc: 0.5772 - val_loss: 0.1988 - val_acc: 0.5721\n",
      "Epoch 11/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1957 - acc: 0.5760 - val_loss: 0.1980 - val_acc: 0.5751\n",
      "Epoch 12/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1953 - acc: 0.5791 - val_loss: 0.1974 - val_acc: 0.5768\n",
      "Epoch 13/16\n",
      "87779/87779 [==============================] - 7s 78us/step - loss: 0.1946 - acc: 0.5806 - val_loss: 0.1969 - val_acc: 0.5773\n",
      "Epoch 14/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1948 - acc: 0.5788 - val_loss: 0.1961 - val_acc: 0.5772\n",
      "Epoch 15/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1940 - acc: 0.5814 - val_loss: 0.1968 - val_acc: 0.5733\n",
      "Epoch 16/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1938 - acc: 0.5821 - val_loss: 0.1968 - val_acc: 0.5751\n",
      "Processing fold 3\n",
      "Train on 87779 samples, validate on 9754 samples\n",
      "Epoch 1/16\n",
      "87779/87779 [==============================] - 8s 93us/step - loss: 0.2166 - acc: 0.5395 - val_loss: 0.2016 - val_acc: 0.5607\n",
      "Epoch 2/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.2001 - acc: 0.5676 - val_loss: 0.1994 - val_acc: 0.5662\n",
      "Epoch 3/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1991 - acc: 0.5696 - val_loss: 0.1993 - val_acc: 0.5681\n",
      "Epoch 4/16\n",
      "87779/87779 [==============================] - 6s 73us/step - loss: 0.1982 - acc: 0.5725 - val_loss: 0.1992 - val_acc: 0.5642\n",
      "Epoch 5/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1985 - acc: 0.5717 - val_loss: 0.1986 - val_acc: 0.5686\n",
      "Epoch 6/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1975 - acc: 0.5731 - val_loss: 0.1981 - val_acc: 0.5704\n",
      "Epoch 7/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1971 - acc: 0.5750 - val_loss: 0.1973 - val_acc: 0.5706\n",
      "Epoch 8/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1968 - acc: 0.5746 - val_loss: 0.1974 - val_acc: 0.5681\n",
      "Epoch 9/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1967 - acc: 0.5764 - val_loss: 0.1988 - val_acc: 0.5657\n",
      "Epoch 10/16\n",
      "87779/87779 [==============================] - 7s 78us/step - loss: 0.1964 - acc: 0.5763 - val_loss: 0.1977 - val_acc: 0.5755\n",
      "Epoch 11/16\n",
      "87779/87779 [==============================] - 7s 75us/step - loss: 0.1958 - acc: 0.5784 - val_loss: 0.1965 - val_acc: 0.5748\n",
      "Epoch 12/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1951 - acc: 0.5796 - val_loss: 0.1977 - val_acc: 0.5736\n",
      "Epoch 13/16\n",
      "87779/87779 [==============================] - 7s 74us/step - loss: 0.1949 - acc: 0.5796 - val_loss: 0.1967 - val_acc: 0.5740\n",
      "Epoch 14/16\n",
      "87779/87779 [==============================] - 7s 79us/step - loss: 0.1943 - acc: 0.5808 - val_loss: 0.1961 - val_acc: 0.5734\n",
      "Epoch 15/16\n",
      "87779/87779 [==============================] - 7s 77us/step - loss: 0.1943 - acc: 0.5821 - val_loss: 0.1999 - val_acc: 0.5705\n",
      "Epoch 16/16\n",
      "87779/87779 [==============================] - 7s 76us/step - loss: 0.1941 - acc: 0.5822 - val_loss: 0.1955 - val_acc: 0.5776\n",
      "Processing fold 4\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 8s 95us/step - loss: 0.2245 - acc: 0.5337 - val_loss: 0.1983 - val_acc: 0.5694\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.2001 - acc: 0.5668 - val_loss: 0.1960 - val_acc: 0.5796\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1997 - acc: 0.5689 - val_loss: 0.1971 - val_acc: 0.5750\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1986 - acc: 0.5716 - val_loss: 0.1946 - val_acc: 0.5764\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1982 - acc: 0.5724 - val_loss: 0.1959 - val_acc: 0.5739\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1981 - acc: 0.5717 - val_loss: 0.1962 - val_acc: 0.5767\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1975 - acc: 0.5736 - val_loss: 0.1941 - val_acc: 0.5772\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1969 - acc: 0.5763 - val_loss: 0.1937 - val_acc: 0.5799\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1968 - acc: 0.5762 - val_loss: 0.1938 - val_acc: 0.5770\n",
      "Epoch 10/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1965 - acc: 0.5749 - val_loss: 0.1940 - val_acc: 0.5779\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1961 - acc: 0.5774 - val_loss: 0.1937 - val_acc: 0.5828\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1959 - acc: 0.5778 - val_loss: 0.1936 - val_acc: 0.5835\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 80us/step - loss: 0.1956 - acc: 0.5795 - val_loss: 0.1924 - val_acc: 0.5831\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1954 - acc: 0.5775 - val_loss: 0.1929 - val_acc: 0.5829\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1950 - acc: 0.5796 - val_loss: 0.1918 - val_acc: 0.5839\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1947 - acc: 0.5807 - val_loss: 0.1927 - val_acc: 0.5824\n",
      "Processing fold 5\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 98us/step - loss: 0.2242 - acc: 0.5339 - val_loss: 0.2004 - val_acc: 0.5637\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.2007 - acc: 0.5654 - val_loss: 0.1973 - val_acc: 0.5699\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1994 - acc: 0.5684 - val_loss: 0.1970 - val_acc: 0.5695\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1988 - acc: 0.5707 - val_loss: 0.1961 - val_acc: 0.5722\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1984 - acc: 0.5716 - val_loss: 0.1958 - val_acc: 0.5748\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1978 - acc: 0.5740 - val_loss: 0.1957 - val_acc: 0.5719\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1975 - acc: 0.5752 - val_loss: 0.1953 - val_acc: 0.5760\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1970 - acc: 0.5748 - val_loss: 0.1950 - val_acc: 0.5749\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1969 - acc: 0.5760 - val_loss: 0.1957 - val_acc: 0.5744\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1966 - acc: 0.5751 - val_loss: 0.1950 - val_acc: 0.5796\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1961 - acc: 0.5776 - val_loss: 0.1952 - val_acc: 0.5751\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1960 - acc: 0.5779 - val_loss: 0.1943 - val_acc: 0.5789\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1958 - acc: 0.5782 - val_loss: 0.1946 - val_acc: 0.5782\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1953 - acc: 0.5782 - val_loss: 0.1944 - val_acc: 0.5794\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1948 - acc: 0.5806 - val_loss: 0.1936 - val_acc: 0.5793\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1949 - acc: 0.5801 - val_loss: 0.1934 - val_acc: 0.5778\n",
      "Processing fold 6\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 104us/step - loss: 0.2250 - acc: 0.5320 - val_loss: 0.2001 - val_acc: 0.5691\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.2004 - acc: 0.5657 - val_loss: 0.1982 - val_acc: 0.5791\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1991 - acc: 0.5677 - val_loss: 0.1964 - val_acc: 0.5811\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1988 - acc: 0.5704 - val_loss: 0.1962 - val_acc: 0.5803\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1982 - acc: 0.5709 - val_loss: 0.1969 - val_acc: 0.5829\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.1980 - acc: 0.5713 - val_loss: 0.1965 - val_acc: 0.5827\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1977 - acc: 0.5723 - val_loss: 0.1957 - val_acc: 0.5833\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1969 - acc: 0.5731 - val_loss: 0.1952 - val_acc: 0.5847\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1966 - acc: 0.5739 - val_loss: 0.1963 - val_acc: 0.5843\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1964 - acc: 0.5760 - val_loss: 0.1948 - val_acc: 0.5878\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1960 - acc: 0.5769 - val_loss: 0.1947 - val_acc: 0.5884\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1959 - acc: 0.5776 - val_loss: 0.1947 - val_acc: 0.5857\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1950 - acc: 0.5793 - val_loss: 0.1939 - val_acc: 0.5892\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1951 - acc: 0.5781 - val_loss: 0.1952 - val_acc: 0.5841\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1949 - acc: 0.5783 - val_loss: 0.1953 - val_acc: 0.5821\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1945 - acc: 0.5805 - val_loss: 0.1934 - val_acc: 0.5864\n",
      "Processing fold 7\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 102us/step - loss: 0.2203 - acc: 0.5348 - val_loss: 0.2002 - val_acc: 0.5724\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.2000 - acc: 0.5675 - val_loss: 0.1980 - val_acc: 0.5740\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1988 - acc: 0.5694 - val_loss: 0.1976 - val_acc: 0.5723\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1986 - acc: 0.5705 - val_loss: 0.1980 - val_acc: 0.5715\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1979 - acc: 0.5723 - val_loss: 0.1963 - val_acc: 0.5753\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1974 - acc: 0.5730 - val_loss: 0.1963 - val_acc: 0.5753\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1969 - acc: 0.5751 - val_loss: 0.1961 - val_acc: 0.5773\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1967 - acc: 0.5751 - val_loss: 0.1958 - val_acc: 0.5778\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1965 - acc: 0.5767 - val_loss: 0.1964 - val_acc: 0.5739\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1966 - acc: 0.5757 - val_loss: 0.1960 - val_acc: 0.5736\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1961 - acc: 0.5774 - val_loss: 0.1971 - val_acc: 0.5757\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1957 - acc: 0.5784 - val_loss: 0.1955 - val_acc: 0.5776\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1952 - acc: 0.5786 - val_loss: 0.1948 - val_acc: 0.5814\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1947 - acc: 0.5797 - val_loss: 0.1946 - val_acc: 0.5819\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1946 - acc: 0.5810 - val_loss: 0.1947 - val_acc: 0.5800\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1944 - acc: 0.5808 - val_loss: 0.1950 - val_acc: 0.5761\n",
      "Processing fold 8\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 102us/step - loss: 0.2205 - acc: 0.5348 - val_loss: 0.1987 - val_acc: 0.5809\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.2003 - acc: 0.5655 - val_loss: 0.1971 - val_acc: 0.5796\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1990 - acc: 0.5683 - val_loss: 0.1967 - val_acc: 0.5816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1985 - acc: 0.5709 - val_loss: 0.1953 - val_acc: 0.5828\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1985 - acc: 0.5703 - val_loss: 0.1951 - val_acc: 0.5849\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 7s 80us/step - loss: 0.1978 - acc: 0.5712 - val_loss: 0.1949 - val_acc: 0.5855\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 81us/step - loss: 0.1974 - acc: 0.5717 - val_loss: 0.1951 - val_acc: 0.5868\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1973 - acc: 0.5733 - val_loss: 0.1953 - val_acc: 0.5830\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1962 - acc: 0.5755 - val_loss: 0.1944 - val_acc: 0.5866\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1961 - acc: 0.5753 - val_loss: 0.1942 - val_acc: 0.5886\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1961 - acc: 0.5758 - val_loss: 0.1941 - val_acc: 0.5855\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1961 - acc: 0.5757 - val_loss: 0.1949 - val_acc: 0.5845\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1954 - acc: 0.5777 - val_loss: 0.1938 - val_acc: 0.5918\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1949 - acc: 0.5800 - val_loss: 0.1944 - val_acc: 0.5838\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 6s 74us/step - loss: 0.1949 - acc: 0.5799 - val_loss: 0.1931 - val_acc: 0.5894\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1943 - acc: 0.5806 - val_loss: 0.1932 - val_acc: 0.5879\n",
      "Processing fold 9\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 101us/step - loss: 0.2302 - acc: 0.5321 - val_loss: 0.2041 - val_acc: 0.5600\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1996 - acc: 0.5688 - val_loss: 0.2036 - val_acc: 0.5649\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1987 - acc: 0.5698 - val_loss: 0.2012 - val_acc: 0.5635\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1980 - acc: 0.5726 - val_loss: 0.2008 - val_acc: 0.5653\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1977 - acc: 0.5727 - val_loss: 0.2004 - val_acc: 0.5710\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1974 - acc: 0.5731 - val_loss: 0.2004 - val_acc: 0.5654\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1968 - acc: 0.5747 - val_loss: 0.1998 - val_acc: 0.5721\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 7s 80us/step - loss: 0.1967 - acc: 0.5759 - val_loss: 0.1993 - val_acc: 0.5753\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1964 - acc: 0.5771 - val_loss: 0.1993 - val_acc: 0.5739\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1961 - acc: 0.5766 - val_loss: 0.1996 - val_acc: 0.5743\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1958 - acc: 0.5783 - val_loss: 0.2001 - val_acc: 0.5722\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 74us/step - loss: 0.1953 - acc: 0.5800 - val_loss: 0.1989 - val_acc: 0.5731\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1954 - acc: 0.5788 - val_loss: 0.1992 - val_acc: 0.5730\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1949 - acc: 0.5785 - val_loss: 0.1998 - val_acc: 0.5720\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1943 - acc: 0.5806 - val_loss: 0.1985 - val_acc: 0.5742\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1940 - acc: 0.5819 - val_loss: 0.1985 - val_acc: 0.5728\n",
      "Processing fold 10\n",
      "Train on 87780 samples, validate on 9753 samples\n",
      "Epoch 1/16\n",
      "87780/87780 [==============================] - 9s 101us/step - loss: 0.2263 - acc: 0.5333 - val_loss: 0.1988 - val_acc: 0.5644\n",
      "Epoch 2/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.2004 - acc: 0.5666 - val_loss: 0.1968 - val_acc: 0.5712\n",
      "Epoch 3/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1992 - acc: 0.5705 - val_loss: 0.1962 - val_acc: 0.5710\n",
      "Epoch 4/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1985 - acc: 0.5712 - val_loss: 0.1963 - val_acc: 0.5740\n",
      "Epoch 5/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1980 - acc: 0.5729 - val_loss: 0.1956 - val_acc: 0.5736\n",
      "Epoch 6/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1978 - acc: 0.5744 - val_loss: 0.1963 - val_acc: 0.5734\n",
      "Epoch 7/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1973 - acc: 0.5750 - val_loss: 0.1952 - val_acc: 0.5750\n",
      "Epoch 8/16\n",
      "87780/87780 [==============================] - 6s 73us/step - loss: 0.1970 - acc: 0.5761 - val_loss: 0.1949 - val_acc: 0.5760\n",
      "Epoch 9/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1968 - acc: 0.5770 - val_loss: 0.1945 - val_acc: 0.5761\n",
      "Epoch 10/16\n",
      "87780/87780 [==============================] - 7s 79us/step - loss: 0.1964 - acc: 0.5784 - val_loss: 0.1940 - val_acc: 0.5761\n",
      "Epoch 11/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1958 - acc: 0.5778 - val_loss: 0.1942 - val_acc: 0.5731\n",
      "Epoch 12/16\n",
      "87780/87780 [==============================] - 7s 75us/step - loss: 0.1956 - acc: 0.5792 - val_loss: 0.1941 - val_acc: 0.5785\n",
      "Epoch 13/16\n",
      "87780/87780 [==============================] - 7s 76us/step - loss: 0.1955 - acc: 0.5788 - val_loss: 0.1943 - val_acc: 0.5790\n",
      "Epoch 14/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1949 - acc: 0.5808 - val_loss: 0.1934 - val_acc: 0.5804\n",
      "Epoch 15/16\n",
      "87780/87780 [==============================] - 7s 78us/step - loss: 0.1944 - acc: 0.5810 - val_loss: 0.1930 - val_acc: 0.5815\n",
      "Epoch 16/16\n",
      "87780/87780 [==============================] - 7s 77us/step - loss: 0.1939 - acc: 0.5834 - val_loss: 0.1939 - val_acc: 0.5811\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=34)\n",
    "\n",
    "splits = []\n",
    "oofpreds = []\n",
    "tspreds = []\n",
    "train = True\n",
    "\n",
    "\n",
    "for i, split in enumerate(kf.split(list(range(cxv1.shape[0])))):\n",
    "    splits.append(split)\n",
    "    print(\"Processing fold {}\".format(i+1))\n",
    "    \n",
    "    model = make_pretrained_model(pretrained_mpath.replace(\"hdf5\", \"json\"))\n",
    "    \n",
    "    tX = [cxv1[split[0]], cxv2[split[0]], cxv3[split[0]], train_rpl_V[split[0]]]\n",
    "    #tX = [train_ctx_V[split[0]], train_rpl_V[split[0]]]\n",
    "    tY = y_train[split[0]]\n",
    "    \n",
    "    vX = [cxv1[split[1]], cxv2[split[1]], cxv3[split[1]], train_rpl_V[split[1]]]\n",
    "    #vX = [train_ctx_V[split[1]], train_rpl_V[split[1]]]\n",
    "    vY = y_train[split[1]]\n",
    "    \n",
    "    gen = 3\n",
    "    class_name = 'pretrained'\n",
    "    net_name = 'pretrained_lstm_rus_fasttext_'+str(i)\n",
    "    \n",
    "    maybe_mkdir(\"./models/gen{}\".format(gen))\n",
    "    maybe_mkdir(\"./models/gen{}/{}\".format(gen, class_name))\n",
    "    \n",
    "    chfilepath = \"./models/gen{}/{}/{}.hdf5\".format(gen, class_name, net_name)\n",
    "    if train:\n",
    "        checkpointer = ModelCheckpoint(chfilepath, save_best_only=True)\n",
    "        histlogger = LossHistory(\"./models/gen{}/{}/{}.csv\".format(gen, class_name, net_name))\n",
    "        json.dump(model.to_json(), open(\"./models/gen{}/{}/{}.json\".format(gen, class_name, net_name), \"w\"))\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                      patience=5, min_lr=0.0001)\n",
    "\n",
    "        h = model.fit(tX, tY, validation_data=(vX, vY), \n",
    "                     batch_size=512, epochs=16,\n",
    "                     verbose=1, callbacks=[checkpointer, histlogger, reduce_lr])\n",
    "    \n",
    "    model.load_weights(chfilepath)\n",
    "    \n",
    "    oofp = model.predict(vX, batch_size=512)\n",
    "\n",
    "    oofpreds.append([oofp, split[1]])\n",
    "    tspreds.append(model.predict([ts_cxv1, ts_cxv2, ts_cxv3, ts_rpl_V], batch_size=512))\n",
    "    #tspreds.append(model.predict([ts_ctx_V, ts_rpl_V], batch_size=512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, save the vocabs for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([voc, rvoc], \n",
    "            open(\"./assets/rus_fasttext.voc\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

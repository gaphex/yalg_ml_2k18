{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aphex/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "import csv\n",
    "import seaborn \n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from text_processing_utils import vectorize, build_vocab, get_embeddings, read_fasttext\n",
    "\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we use the trained models to generate features for the ensembling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    columns = ['context_id','context_2','context_1','context_0','reply_id','reply','label','confidence']\n",
    "\n",
    "    test_df = pd.read_csv(\"./data/final.tsv\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "    train_df = pd.read_csv(\"./data/train.tsv\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    train_df = train_df.fillna(\"\")\n",
    "    test_df = test_df.fillna(\"\")\n",
    "\n",
    "    test_df.columns = columns[:-2]\n",
    "    train_df.columns = columns\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = []\n",
    "for l in train_df['label'].tolist():\n",
    "    if l == 'good':\n",
    "        nl.append(1)\n",
    "    elif l == 'neutral':\n",
    "        nl.append(0.5)\n",
    "    elif l == 'bad':\n",
    "        nl.append(0)\n",
    "        \n",
    "y_train = np.array(nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the word-to-id mappings used to train different supervised models. These are needed for reproducability, but are useless without the actual model weights (not included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VD = pickle.load(open(\"./assets/repr_vocs.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_w2v import tokenizers\n",
    "\n",
    "def tokenize_split(sents):\n",
    "    return [tokenizers.tokenize_split(s) for s in sents]\n",
    "\n",
    "def tokenize_char(sents):\n",
    "    return [str(s) for s in sents]\n",
    "\n",
    "def make_preds(model, wpaths, vocab, tokenizer, MAXLEN):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=34)\n",
    "\n",
    "    splits = []\n",
    "    oofpreds = []\n",
    "    tspreds = []\n",
    "    \n",
    "    cxv1 = vectorize(tokenizer(train_df['context_2'].tolist()), vocab, max_len=MAXLEN)\n",
    "    cxv2 = vectorize(tokenizer(train_df['context_1'].tolist()), vocab, max_len=MAXLEN)\n",
    "    cxv3 = vectorize(tokenizer(train_df['context_0'].tolist()), vocab, max_len=MAXLEN)\n",
    "    train_rpl_V = vectorize(tokenizer(train_df['reply'].tolist()), vocab, max_len=MAXLEN)\n",
    "    \n",
    "    ts_cxv1 = vectorize(tokenizer(test_df['context_2'].tolist()), vocab, max_len=MAXLEN)\n",
    "    ts_cxv2 = vectorize(tokenizer(test_df['context_1'].tolist()), vocab, max_len=MAXLEN)\n",
    "    ts_cxv3 = vectorize(tokenizer(test_df['context_0'].tolist()), vocab, max_len=MAXLEN)\n",
    "    ts_rpl_V = vectorize(tokenizer(test_df['reply'].tolist()), vocab, max_len=MAXLEN)\n",
    "\n",
    "    scores = []\n",
    "    for i, split in enumerate(kf.split(list(range(cxv1.shape[0])))):\n",
    "        splits.append(split)\n",
    "\n",
    "        tX = [cxv1[split[0]], cxv2[split[0]], cxv3[split[0]], train_rpl_V[split[0]]]\n",
    "        tY = y_train[split[0]]\n",
    "\n",
    "        vX = [cxv1[split[1]], cxv2[split[1]], cxv3[split[1]], train_rpl_V[split[1]]]\n",
    "        vY = y_train[split[1]]\n",
    "\n",
    "        if len(wpaths) == 10:\n",
    "            model.load_weights(wpaths[i])\n",
    "        else:\n",
    "            model.load_weights(wpaths)\n",
    "\n",
    "        oofp = model.predict(vX, batch_size=512)\n",
    "        scores.append(mean_squared_error(vY, oofp))\n",
    "        oofpreds.append([oofp, split[1]])\n",
    "        tspreds.append(model.predict([ts_cxv1, ts_cxv2, ts_cxv3, ts_rpl_V], batch_size=512))\n",
    "    print(np.mean(scores))    \n",
    "    return splits, oofpreds, tspreds, [ts_cxv1, ts_cxv2, ts_cxv3, ts_rpl_V]\n",
    "\n",
    "def make_pretrained_model(mpath, seqlen):\n",
    "    model = model_from_json(json.load(open(mpath)))\n",
    "\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(\"deep_sim_net\").get_output_at(1))\n",
    "    lr = model.get_layer(\"deep_sim_net\").get_output_at(0)\n",
    "\n",
    "    for layer in intermediate_layer_model.layers[:-1]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    SEQ_LEN = seqlen\n",
    "\n",
    "    inp_ctx1 = Input(shape=(SEQ_LEN,))\n",
    "    inp_ctx2 = Input(shape=(SEQ_LEN,))\n",
    "    inp_ctx3 = Input(shape=(SEQ_LEN,))\n",
    "    inp_rpl = Input(shape=(SEQ_LEN,))\n",
    "\n",
    "    dns = intermediate_layer_model([inp_ctx1, inp_ctx2, inp_ctx3, inp_rpl])\n",
    "    dns_out = Dense(1)(dns)\n",
    "\n",
    "    fin_model = Model(inputs=[inp_ctx1, inp_ctx2, inp_ctx3, inp_rpl], outputs=dns_out)\n",
    "    fin_model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return fin_model\n",
    "\n",
    "def assert_equal(pack, path):\n",
    "    print(\"checking ... \", end=\"\")\n",
    "    tf = pickle.load(open(path, \"rb\"))\n",
    "    for i, j in zip(pack[1], tf[1]):\n",
    "        assert(np.allclose(i[0], j[0]))\n",
    "    print(\"valid\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model, trained on russian subtitles using fasttext embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aphex/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "model = make_pretrained_model(\"./models/gen3/memnet/lstm_rus_fasttext.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_voc = pickle.load(open(\"./assets/rus_fasttext.voc\", \"rb\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 518115.87it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 439514.60it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 194084.08it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 355982.56it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 244108.69it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 442013.42it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 376094.54it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 198107.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19421859127633517\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model, \n",
    "           sorted(glob(\"./models/gen3/pretrained/pretrained_lstm_rus_fasttext_*.hdf5\")), \n",
    "           my_voc, tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./features/supervised/rus_fasttext.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 7 models I used to make the final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russian subtitles + CBOW word embeddings trained on OPUS using scripts from ./gensim_w2v directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aphex/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_sd_lstm_dsm_binary_paranoid_xlarge.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 332026.64it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 423217.24it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 379208.62it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 256113.74it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 340717.12it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 354495.28it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 274571.33it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 259408.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19139334832704172\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model, \n",
    "           sorted(glob(\"./models/gen2/pretrained/_tunedmemnet_xlarge_myembs_regr_*.hdf5\")), \n",
    "           VD['cbow_1M_ppc_big'][0], tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "assert_equal(ppack, \"./reproducable/supervised/tunedmemnet_xlarge_myembs_regr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/tunedmemnet_xlarge_myembs_regr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russian subtitles + fasttext word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_sd_gru_dsm_binary_paranoid_big.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 314462.78it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 285731.98it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 369019.03it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 214522.28it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 319658.77it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 264896.02it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 254760.20it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 239416.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18832487898291178\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model,\n",
    "           sorted(glob(\"./models/gen2/pretrained/_tunedmemnet_biggru_ft_regr_*.hdf5\")), \n",
    "           VD['cc.ru.300_big'][0], tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "assert_equal(ppack, \"./reproducable/supervised/tunedmemnet_biggru_ft_regr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/tunedmemnet_biggru_ft_regr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russian subtitles - char-level model (no pretrained embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_cl_sd_lstm_dsm_binary_paranoid_xlarge.json\", 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 194610.73it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 119922.76it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 127411.66it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 115021.34it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 199373.76it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 129825.52it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 105224.51it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 132692.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19324107112287126\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model,\n",
    "           sorted(glob(\"./models/gen2/pretrained/pretrained_memnet_cl_sd_lstm_dsm_binary_paranoid_xlarge*.hdf5\")), \n",
    "           VD['clevel_big'][0], tokenize_char, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "assert_equal(ppack, \"./reproducable/supervised/tuned_memnet_cl_sd_lstm_dsm_binary_paranoid_xlarge.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/tuned_memnet_cl_sd_lstm_dsm_binary_paranoid_xlarge.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Russian subtitles lemmatized with mystem + CBOW word embeddings trained on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_sd_gru_dsm_binary_paranoid_lemma.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "stemmer = Mystem()\n",
    "\n",
    "def lemmatize(t):\n",
    "    return ''.join(stemmer.lemmatize(t)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['context_2'] = [lemmatize(t) for t in train_df['context_2']]\n",
    "train_df['context_1'] = [lemmatize(t) for t in train_df['context_1']]\n",
    "train_df['context_0'] = [lemmatize(t) for t in train_df['context_0']]\n",
    "train_df['reply'] = [lemmatize(t) for t in train_df['reply']]\n",
    "\n",
    "test_df['context_2'] = [lemmatize(t) for t in test_df['context_2']]\n",
    "test_df['context_1'] = [lemmatize(t) for t in test_df['context_1']]\n",
    "test_df['context_0'] = [lemmatize(t) for t in test_df['context_0']]\n",
    "test_df['reply'] = [lemmatize(t) for t in test_df['reply']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 489586.79it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 247923.41it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 224727.57it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 342395.39it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 486646.63it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 410380.30it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 357292.85it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 341661.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19039269212295565\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model,\n",
    "                   sorted(glob(\"./models/gen2/pretrained/sd_memnet_lemma_mycbow_regr*.hdf5\")), \n",
    "                   VD['lemma'][0], tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "assert_equal(ppack, \"./reproducable/supervised/memnet_lemma_mycbow_regr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/memnet_lemma_mycbow_regr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English subtitles + fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aphex/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_lstm_eng_ft.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pickle.load(open(\"./assets/translations/english_preprocessed.pkl\",\"rb\"))\n",
    "#trans1 = pickle.load(open(\"./assets/translations/fin.eng.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    for col in ['context_2','context_1','context_0','reply']:\n",
    "        df[col] = [trans[t] for t in df[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 329223.35it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 276766.89it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 269277.79it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 259398.30it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 339675.88it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 299523.35it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 265300.70it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 268936.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19341991963321165\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model,\n",
    "                   sorted(glob(\"./models/gen2/pretrained/pretrained_memnet_lstm_eng_ft_*.hdf5\")), \n",
    "                   VD['eng_ft'][0], tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "t = assert_equal(ppack, \"./reproducable/supervised/pretrained_memnet_lstm_eng_ft.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/pretrained_memnet_lstm_eng_ft.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English subtitles + glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_lstm_eng_glove.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 295888.96it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 419124.86it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 243955.89it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 239618.99it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 309389.55it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 267594.01it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 240605.15it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 248212.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19370656250713264\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model,\n",
    "                   sorted(glob(\"./models/gen2/pretrained/pretrained_memnet_lstm_eng_gl_*.hdf5\")), \n",
    "                   VD['eng_glove'][0], tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "t = assert_equal(ppack, \"./reproducable/supervised/pretrained_memnet_lstm_eng_glove.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/pretrained_memnet_lstm_eng_glove.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spanish subtitles + fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pickle.load(open(\"./assets/translations/all_esp.finn.pkl\",\"rb\"))\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    for col in ['context_2','context_1','context_0','reply']:\n",
    "        df[col] = [trans[t] for t in df[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pretrained_model(\"./models/gen2/memnet/memnet_lstm_esp_fst.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:00<00:00, 315864.74it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 279702.09it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 266336.87it/s]\n",
      "100%|██████████| 97533/97533 [00:00<00:00, 410264.61it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 318784.14it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 291721.29it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 270777.40it/s]\n",
      "100%|██████████| 104834/104834 [00:00<00:00, 399855.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1916239633483493\n"
     ]
    }
   ],
   "source": [
    "ppack = make_preds(model,\n",
    "                   sorted(glob(\"./models/gen2/pretrained/pretrained_memnet_lstm_esp_ft_*.hdf5\")), \n",
    "                   VD['esp_ft'][0], tokenize_split, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking ... valid\n"
     ]
    }
   ],
   "source": [
    "t = assert_equal(ppack, \"./reproducable/supervised/pretrained_memnet_lstm_esp_fasttext.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ppack, open(\"./final/supervised/pretrained_memnet_lstm_esp_fasttext.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute assorted unsupervised NLP features.\n",
    "\n",
    "Make sure you install wmd-relax from https://github.com/src-d/wmd-relax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/aphex/Downloads/wmd-relax\")\n",
    "import libwmdrelax\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "from text_processing_utils import read_fasttext\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "import csv\n",
    "import seaborn \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from gensim.models import FastText\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "stemmer = Mystem()\n",
    "\n",
    "def lemmatize(t):\n",
    "    return ''.join(stemmer.lemmatize(t)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacySimilarityHook(object):\n",
    "        \"\"\"\n",
    "        This guy is needed for the integration with `spaCy <https://spacy.io>`_.\n",
    "        Use it like this:\n",
    "        ::\n",
    "           nlp = spacy.load('en', create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "        It defines :func:`~wmd.WMD.SpacySimilarityHook.compute_similarity()` \\\n",
    "        method which is called by spaCy over pairs of\n",
    "        `documents <https://spacy.io/docs/api/doc>`_.\n",
    "        .. automethod:: wmd::WMD.SpacySimilarityHook.__init__\n",
    "        \"\"\"\n",
    "        def __init__(self, nlp, **kwargs):\n",
    "            \"\"\"\n",
    "            Initializes a new instance of SpacySimilarityHook class.\n",
    "            :param nlp: `spaCy language object <https://spacy.io/docs/api/language>`_.\n",
    "            :param ignore_stops: Indicates whether to ignore the stop words.\n",
    "            :param only_alpha: Indicates whether only alpha tokens must be used.\n",
    "            :param frequency_processor: The function which is applied to raw \\\n",
    "                                        token frequencies.\n",
    "            :type ignore_stops: bool\n",
    "            :type only_alpha: bool\n",
    "            :type frequency_processor: callable\n",
    "            \"\"\"\n",
    "            \n",
    "            self.lower = kwargs.get(\"lower\", False)\n",
    "            self.nlp = nlp\n",
    "            self.ignore_stops = kwargs.get(\"ignore_stops\", True)\n",
    "            self.only_alpha = kwargs.get(\"only_alpha\", True)\n",
    "            self.frequency_processor = kwargs.get(\n",
    "                \"frequency_processor\", lambda t, f: np.log(1 + f))\n",
    "\n",
    "        def __call__(self, doc):\n",
    "            doc.user_hooks[\"similarity\"] = self.compute_similarity\n",
    "            doc.user_span_hooks[\"similarity\"] = self.compute_similarity\n",
    "\n",
    "        def compute_similarity(self, doc1, doc2):\n",
    "            \"\"\"\n",
    "            Calculates the similarity between two spaCy documents. Extracts the\n",
    "            nBOW from them and evaluates the WMD.\n",
    "            :return: The calculated similarity.\n",
    "            :rtype: float.\n",
    "            \"\"\"\n",
    "            if self.lower:\n",
    "                doc1 = doc1.lower()\n",
    "                doc2 = doc2.lower()\n",
    "            \n",
    "            doc1 = self._convert_document(doc1)\n",
    "            doc2 = self._convert_document(doc2)\n",
    "            vocabulary = {\n",
    "                w: i for i, w in enumerate(sorted(set(doc1).union(doc2)))}\n",
    "            \n",
    "            #print(vocabulary)\n",
    "            w1 = self._generate_weights(doc1, vocabulary)\n",
    "            w2 = self._generate_weights(doc2, vocabulary)\n",
    "            \n",
    "            #print(w1,w2, vocabulary)\n",
    "            if hasattr(self.nlp, \"vector_size\"):\n",
    "                evec = np.zeros((len(vocabulary), self.nlp.vector_size),\n",
    "                                   dtype=np.float32)\n",
    "            else:\n",
    "                evec = np.zeros((len(vocabulary), 300),\n",
    "                                   dtype=np.float32)\n",
    "            for w, i in vocabulary.items():\n",
    "                evec[i] = self.nlp[w]\n",
    "            evec_sqr = (evec * evec).sum(axis=1)\n",
    "            dists = evec_sqr - 2 * evec.dot(evec.T) + evec_sqr[:, np.newaxis]\n",
    "            dists[dists < 0] = 0\n",
    "            dists = np.sqrt(dists)\n",
    "            try:\n",
    "                return libwmdrelax.emd(w1, w2, dists)\n",
    "            except (RuntimeError, MemoryError):\n",
    "                return 100.0\n",
    "            \n",
    "        def compute_similarity_batch(self, doc1, docs):\n",
    "            return np.array([self.compute_similarity(doc1, doc) for doc in docs])\n",
    "\n",
    "        def _convert_document(self, doc):\n",
    "            words = defaultdict(int)\n",
    "            for t in nltk.word_tokenize(doc):\n",
    "                if t in self.nlp:\n",
    "                    words[t] += 1\n",
    "            return {t: self.frequency_processor(t, v) for t, v in words.items()}\n",
    "\n",
    "        def _generate_weights(self, doc, vocabulary):\n",
    "            w = np.zeros(len(vocabulary), dtype=np.float32)\n",
    "            for t, v in doc.items():\n",
    "                w[vocabulary[t]] = v\n",
    "            w /= w.sum()\n",
    "            return w\n",
    "        \n",
    "def get_wmd_similarities(w2v_model, dframe, lemma=False):\n",
    "\n",
    "    ssh = SpacySimilarityHook(w2v_model)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for row in tqdm(dframe.iterrows(), total=len(dframe)):\n",
    "        t = row[1]\n",
    "        if lemma:\n",
    "            preds.append([ssh.compute_similarity(lemmatize(t['context_2']), lemmatize(t['reply'])),\n",
    "                          ssh.compute_similarity(lemmatize(t['context_1']), lemmatize(t['reply'])),\n",
    "                          ssh.compute_similarity(lemmatize(t['context_0']), lemmatize(t['reply']))])\n",
    "        else:\n",
    "            preds.append([ssh.compute_similarity(t['context_2'], t['reply']),\n",
    "                          ssh.compute_similarity(t['context_1'], t['reply']),\n",
    "                          ssh.compute_similarity(t['context_0'], t['reply'])])\n",
    "        \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# russian fasttext\n",
    "# russian araneum http://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz\n",
    "# russian OPUS gensim CBOW model\n",
    "# russian lemmatized OPUS gensim CBOW model\n",
    "\n",
    "word_vectors = {\n",
    "    'fasttext': read_fasttext(\"./assets/cc.ru.300.vec\"),\n",
    "    'araneum': FastText.load(\"./assets/araneum_none_fasttextcbow_300_5_2018.model\"),\n",
    "    'my_ppc': Word2Vec.load(\"./assets/cbow_1M.w2v\"),\n",
    "    'my_ppc_lemma': Word2Vec.load(\"./assets/cbow_lemma.w2v\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 43/97533 [00:00<03:49, 424.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aphex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 97533/97533 [02:09<00:00, 753.93it/s]\n",
      "100%|██████████| 104834/104834 [02:19<00:00, 750.34it/s]\n",
      "  0%|          | 0/97533 [00:00<?, ?it/s]/home/aphex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/aphex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:64: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  0%|          | 33/97533 [00:00<04:56, 328.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "araneum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [03:40<00:00, 442.28it/s]\n",
      "100%|██████████| 104834/104834 [03:57<00:00, 441.26it/s]\n",
      "  0%|          | 58/97533 [00:00<02:49, 574.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_ppc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [02:27<00:00, 660.77it/s]\n",
      "100%|██████████| 104834/104834 [02:39<00:00, 658.18it/s]\n",
      "  0%|          | 0/97533 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_ppc_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [07:00<00:00, 231.83it/s]\n",
      "100%|██████████| 104834/104834 [07:36<00:00, 229.73it/s]\n"
     ]
    }
   ],
   "source": [
    "wmd_r = {}\n",
    "for wv in word_vectors:\n",
    "    print(wv)\n",
    "    wmd_r[wv] = [get_wmd_similarities(word_vectors[wv], train_df, lemma=('lemma' in wv)),\n",
    "                 get_wmd_similarities(word_vectors[wv], test_df, lemma=('lemma' in wv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in wmd_r:\n",
    "    pickle.dump(wmd_r[w], open(\"./features/unsupervised/wmd_{}.pkl\".format(w), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltexts = set()\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    for col in ['context_2','context_1','context_0', 'reply']:\n",
    "        alltexts.update(df[col].tolist())\n",
    "        \n",
    "alltexts = list(alltexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_sims(vect, dframe):\n",
    "    c2 = vect.transform(dframe['context_2'].tolist())\n",
    "    c1 = vect.transform(dframe['context_1'].tolist())\n",
    "    c0 = vect.transform(dframe['context_0'].tolist())\n",
    "\n",
    "    r0 = vect.transform(dframe['reply'].tolist())\n",
    "\n",
    "    c2r0 = paired_cosine_distances(c2,r0)\n",
    "    c1r0 = paired_cosine_distances(c1,r0)\n",
    "    c0r0 = paired_cosine_distances(c0,r0)\n",
    "\n",
    "    preds = np.hstack([c2r0.reshape(-1,1), c1r0.reshape(-1,1), c0r0.reshape(-1,1)])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tf_r = {}\n",
    "for i in range(3):\n",
    "    tfv = TfidfVectorizer(ngram_range=(1,i), max_features=200000)\n",
    "    tfv.fit(alltexts)\n",
    "    tf_r[i] = [get_tf_sims(tfv, train_df), get_tf_sims(tfv, test_df)]\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tf_r:\n",
    "    pickle.dump(tf_r[w], open(\"./features/unsupervised/tfv_{}.pkl\".format(w), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def encode(text, w2v, lemma):\n",
    "    if lemma:\n",
    "        text = lemmatize(text)\n",
    "    toks = nltk.word_tokenize(text)\n",
    "    empty = np.zeros(300)\n",
    "    ftoks = [tok for tok in toks if tok in w2v]\n",
    "    if not len(ftoks):\n",
    "        return empty\n",
    "    else:\n",
    "        return np.mean([w2v[tok] for tok in ftoks], axis=0)\n",
    "    \n",
    "def get_cossim(t1,t2):\n",
    "    return cosine_similarity(encode(t1, w2v).reshape(1,-1), encode(t2, w2v).reshape(1,-1))[0][0]\n",
    "\n",
    "def get_avgw2v_sims(w2v_model, dframe, lemma=False):\n",
    "    ct2 = np.array([encode(t, w2v_model, lemma) for t in dframe['context_2']])\n",
    "    ct1 = np.array([encode(t, w2v_model, lemma) for t in dframe['context_1']])\n",
    "    ct0 = np.array([encode(t, w2v_model, lemma) for t in dframe['context_0']])\n",
    "    \n",
    "    rp = np.array([encode(t, w2v_model, lemma) for t in dframe['reply']])\n",
    "    \n",
    "    c2r0 = paired_cosine_distances(ct2,rp)\n",
    "    c1r0 = paired_cosine_distances(ct1,rp)\n",
    "    c0r0 = paired_cosine_distances(ct0,rp)\n",
    "\n",
    "    preds = np.hstack([c2r0.reshape(-1,1), c1r0.reshape(-1,1), c0r0.reshape(-1,1)])\n",
    "    return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aphex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/home/aphex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "araneum False\n",
      "my_ppc False\n",
      "my_ppc_lemma True\n"
     ]
    }
   ],
   "source": [
    "avw_r = {}\n",
    "for wv in word_vectors:\n",
    "    \n",
    "    avw_r[wv] = [get_avgw2v_sims(word_vectors[wv], test_df, 'lemma' in wv), \n",
    "                 get_avgw2v_sims(word_vectors[wv], train_df, 'lemma' in wv)]\n",
    "    print(wv, 'lemma' in wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in avw_r:\n",
    "    pickle.dump(avw_r[w][::-1], open(\"./features/unsupervised/avw_{}.pkl\".format(w), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "stops = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_match_share(s1, s2, lemma):\n",
    "    if lemma:\n",
    "        s1 = lemmatize(str(s1))\n",
    "        s2 = lemmatize(str(s2))\n",
    "\n",
    "    q1words = [word for word in str(s1).lower().split() if word not in stops and word not in string.punctuation]\n",
    "    q2words = [word for word in str(s2).lower().split() if word not in stops and word not in string.punctuation]\n",
    "\n",
    "\n",
    "    if len(q1words) == 0 and len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_words = len(set(q1words)&set(q2words))\n",
    "\n",
    "    R = 2*shared_words/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def get_wms(dframe, lemma=False):\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for row in tqdm(dframe.iterrows(), total=len(dframe)):\n",
    "        t = row[1]\n",
    "        preds.append([word_match_share(t['context_2'], t['reply'], lemma),\n",
    "                      word_match_share(t['context_1'], t['reply'], lemma),\n",
    "                      word_match_share(t['context_0'], t['reply'], lemma)])\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:13<00:00, 7350.82it/s]\n",
      "100%|██████████| 104834/104834 [00:14<00:00, 7349.10it/s]\n"
     ]
    }
   ],
   "source": [
    "wms_stops = [get_wms(train_df), get_wms(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:10<00:00, 9742.70it/s]\n",
      "100%|██████████| 104834/104834 [00:10<00:00, 9999.10it/s] \n"
     ]
    }
   ],
   "source": [
    "stops = []\n",
    "wms_nostops = [get_wms(train_df), get_wms(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(wms_stops, open(\"./features/unsupervised/wms_stops.pkl\",\"wb\"))\n",
    "pickle.dump(wms_nostops, open(\"./features/unsupervised/wms_nostops.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(wms_lemma, open(\"./reproducable/unsupervised/wms_lemma.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(dframe):\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for row in tqdm(dframe.iterrows(), total=len(dframe)):\n",
    "        t = row[1]\n",
    "        preds.append([len(t['context_2']), len(t['context_2'].split()),\n",
    "                      len(t['context_1']), len(t['context_1'].split()),\n",
    "                      len(t['context_0']), len(t['context_0'].split()),\n",
    "                      len(t['reply']), len(t['reply'].split())])\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:09<00:00, 10552.50it/s]\n",
      "100%|██████████| 104834/104834 [00:10<00:00, 10481.80it/s]\n"
     ]
    }
   ],
   "source": [
    "lengths = [get_lengths(train_df), get_lengths(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lengths, open(\"./features/unsupervised/lengths.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fuzzy dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuzzy_dists(dframe):\n",
    "    preds = []\n",
    "    for row in tqdm(dframe.iterrows(), total=len(dframe)):\n",
    "        t = row[1]\n",
    "        preds.append([fuzz.ratio(t['context_2'], t['reply']),\n",
    "                      fuzz.ratio(t['context_1'], t['reply']),\n",
    "                      fuzz.ratio(t['context_0'], t['reply'])])\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:09<00:00, 9958.37it/s]\n",
      "100%|██████████| 104834/104834 [00:10<00:00, 10251.96it/s]\n"
     ]
    }
   ],
   "source": [
    "fuzz = [get_fuzzy_dists(train_df), get_fuzzy_dists(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fuzz, open(\"./features/unsupervised/fuzz.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qs = train_df['context_0']\n",
    "test_qs = test_df['context_0']\n",
    "rpc = Counter(train_qs.tolist()+test_qs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markdown_ftrs(dframe):\n",
    "    fs = []\n",
    "    for col in ['context_0','reply']:\n",
    "        ser = dframe[col]\n",
    "\n",
    "        qmarks = ser.apply(lambda x: '?' in x)\n",
    "        fullstop = ser.apply(lambda x: '.' in x)\n",
    "        capitals = ser.apply(lambda x: sum([y in string.punctuation for y in x]))\n",
    "        numbers = ser.apply(lambda x: sum([y.isdigit() for y in x]))\n",
    "        rfreq = ser.apply(lambda x: rpc[x])\n",
    "\n",
    "        fts = np.array([qmarks.as_matrix().astype('int'), fullstop.as_matrix().astype('int'), \n",
    "                              capitals.as_matrix().astype('int'),numbers.as_matrix().astype('int'), \n",
    "                              rfreq.as_matrix().astype('int')])\n",
    "        fs.append(fts.T)\n",
    "    return np.hstack(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = [get_markdown_ftrs(train_df), get_markdown_ftrs(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mdf, open(\"./features/unsupervised/markdown.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rogue_scores(dframe):\n",
    "    preds = []\n",
    "\n",
    "    for row in tqdm(dframe.iterrows(), total=len(dframe)):\n",
    "        t = row[1]\n",
    "\n",
    "        scores = rouge.get_scores(t['context_0'], t['reply'])\n",
    "\n",
    "        preds.append([scores[0]['rouge-1'][t] for t in ['f','p','r']] + \\\n",
    "    [scores[0]['rouge-2'][t] for t in ['f','p','r']] + \\\n",
    "    [scores[0]['rouge-l'][t] for t in ['f','p','r']])\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97533/97533 [00:12<00:00, 7619.26it/s]\n",
      "100%|██████████| 104834/104834 [00:13<00:00, 7562.71it/s]\n"
     ]
    }
   ],
   "source": [
    "rog = [get_rogue_scores(train_df), get_rogue_scores(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rog, open(\"./features/unsupervised/rogue.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anl_cache = {t:stemmer.analyze(t) for t in tqdm(alltexts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propn_scores(dframe):\n",
    "    preds = []\n",
    "    for row in tqdm(dframe.iterrows(), total=len(dframe)):\n",
    "        t = row[1]\n",
    "        preds.append([sum([1 for t in anl_cache[t['context_2']] if 'имя' in str(t)]),\n",
    "                     sum([1 for t in anl_cache[t['context_1']] if 'имя' in str(t)]),\n",
    "                     sum([1 for t in anl_cache[t['context_0']] if 'имя' in str(t)]),\n",
    "                     sum([1 for t in anl_cache[t['reply']] if 'имя' in str(t)])])\n",
    "\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "propn = [get_propn_scores(train_df), get_propn_scores(test_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(propn, open(\"./reproducable/unsupervised/propn.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
